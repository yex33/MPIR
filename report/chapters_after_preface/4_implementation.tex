\chapter{Implementation}
\label{cha:implementation}

The solver implementation combines modern C++ template programming with
numerical linear algebra techniques to support mixed-precision iterative
refinement on sparse systems. Its overall design emphasizes three guiding
principles:

\begin{description}
\item[Interface compatibility] The solver interface mimics that of Eigen’s sparse
  solvers \cite{noauthor_eigen_nodate}, ensuring easy adoption and familiarity
  for users already working within this ecosystem.
\item[Precision flexibility] All arithmetic operations are templated over precision
  types, enabling seamless use of IEEE 754 and extended floating-point formats
  in mixed-precision workflows.
\item[Modularity] Numerical kernels such as sparse matrix-vector multiplication,
  triangular solves, and vector operations are implemented in a modular
  BLAS-like layer, separating solver logic from low-level computations.
\end{description}

This chapter presents the solver interface design, the template and concept
infrastructure supporting mixed precision, extensions to an external library for
extended floating-point types, and the modular BLAS-like kernels. It then
details the key numerical algorithms implemented: the preconditioned GMRES
kernel, the fine-grained parallel ILU factorization, and the mixed-precision
iterative refinement driver that integrates them into a complete solver
pipeline.

\section{Solver Interface Design}
\label{sec:solv-inetrf-design}

A central design objective is to expose an interface resembling that of Eigen's
sparse solvers. The solver is implemented as a class template parameterized by
three precision types:
\begin{center}
  \texttt{Solver<UF, UW, UR>}
\end{center}
where the template parameters are defined as below:
\begin{description}
\item[\texttt{UF}] factorization precision, used for incomplete LU computations,
\item[\texttt{UW}] working precision, used within GMRES iterations; this is also the
  precision of the input data and the result;
\item[\texttt{UR}] residual precision, used for residual evaluation during iterative
  refinement.
\end{description}

The solver class owns a copy of the input matrix, which can be passed by value
or moved into the solver instance to avoid unnecessary copies. The interface
provides two key methods:
\begin{description}
\item[\texttt{compute()}] accepts a sparse matrix in compressed storage format and performs
  ILU factorization in precision UF;
\item[\texttt{solve()}] accepts a right-hand side vector \(\vec{b}\), executes the
  mixed-precision iterative refinement algorithm, and returns the solution
  vector \(\vec{x}\).
\end{description}

\section{C++ Templates and Mixed Precision Concepts}
\label{sec:c++-templates-mixed}

The implementation makes extensive use of C++ templates to support type-generic
arithmetic. To ensure correctness and prevent invalid instantiations, variadic
concepts are introduced.

The \texttt{FloatingPoint} concept ensures that all participating types are valid
floating-point types, including both IEEE 754 types (\texttt{float},
\texttt{double}) and extended types (\texttt{dd\_real} for quadruple precision,
\texttt{qd\_real} for octuple precision) from the QD
\cite{david_bailey_bl-highprecisionqd_2025} library.

To enforce precision hierarchies, the \texttt{PartialOrdered} concept requires a
monotonic ordering of machine epsilon values:
\begin{center}
  \(\epsilon(\texttt{UF}) \ge \epsilon(\texttt{UW}) \ge \epsilon(\texttt{UR}),\)
\end{center}
where \(\epsilon(\texttt{T}) = \texttt{std::numeric\_limits<T>::epsilon()}\) is the machine
epsilon of precision type \texttt{T}.

The \texttt{Refinable} concept combines the two concepts above and guarantees that
only valid triples (\texttt{UF}, \texttt{UW}, \texttt{UR}) can instantiate the
solver. It ensures that factorization occurs in the lowest precision and
residuals are always computed in the highest precision.

\section{Extensions to the QD Library}
\label{sec:extens-qd-libr}

The QD package provides quadruple and octuple precision through the data types
\texttt{dd\_real} and \texttt{qd\_real}, respectively. However, it lacks certain
functionality required for modern C++ interoperability. To integrate QD
seamlessly into the solver, several extensions were implemented.

\begin{description}
\item[\texttt{std::numeric\_limits} Specializations] The corresponding machine epsilon,
  maximum, minimum values were made \texttt{constexpr}, allowing QD types to
  participate in template metaprogramming and concept checks.
\item[Explicit Conversion Functions] Bidirectional explicit conversion functions
  were implemented between QD types and both built-in IEEE types
  (\texttt{float}, \texttt{double}) and fixed-width standard types
  (\texttt{std::float16\_t}, \texttt{std::float32\_t},
  \texttt{std::float64\_t}).
\item [Formatting and I/O] Implementations of \texttt{std::formatter} were added,
  enabling direct use of \texttt{std::print} and \texttt{std::format} with QD
  types.
\end{description}

\section{Modular BLAS-like Kernels}
\label{sec:modular-blas-like}

To decouple solver logic from low-level numerical kernels, all matrix and vector
operations are organized into a separate module, following the philosophy of
BLAS \cite{blackford_2002_updated}.

Each kernel is templated not only on the element type but also on the
input/output type. For example, this design allows multiplying a Compressed
Sparse Column (CSC) matrix stored in \texttt{single} precision by a dense
\texttt{double} precision vector, producing results in \texttt{double} while the
arithmetic is carried out in \texttt{dd\_real} (quadruple). Explicit
\texttt{std::static\_cast} ensures that precision hierarchies are respected.

\section{Implemented Algorithms}
\label{sec:impl-algo}

The solver integrates three core algorithmic components: a preconditioned GMRES
method, an incomplete LU factorization (ILU), and an iterative refinement (IR)
driver. Each component is fully templated in C++ and implemented with explicit
attention to mixed-precision arithmetic.

\subsection{Preconditioned GMRES}
\label{sec:preconditioned-gmres}

Algorithm \ref{algo:gmres} implements the preconditioned GMRES procedure, following
the algorithm presented in \cite{lindquist_improving_2020}. In this work, the
restarting mechanism from the original version is omitted, since the IR driver
described in Section~\ref{sec:mixed-precision-ir} naturally enforces a restart
after each refinement cycle. As a result, the GMRES kernel functions as a
building block within the mixed-precision IR pipeline, focusing solely on Krylov
subspace construction, orthogonalization, and residual minimization in the
preconditioned space.

\begin{singlespace}
  \begin{algorithm}[h]
    \caption{GMRES with left preconditioning, given matrix \(\matr{A} \in \mathbb{R}^{n \times
        n}\), right-hand side vector \(\vec{b} \in \mathbb{R}^n\), optional
      initial guess \(\vec{x}_0 \in \mathbb{R}^n\) (if not given, \(\vec{x}_0\)
      is the zero vector), preconditioner \(\matr{M}^{-1} \approx
      \matr{A}^{-1}\), tolerance \(\epsilon\), maximum number of inner
      iterations \(n_{\text{inner}}\)}
    \label{algo:gmres}
    \begin{algorithmic}[1]
      \State \(\vec{z} \gets \vec{b} - \matr{A}\vec{x}_0\) \Comment{Compute residual}
      \State \(\vec{r} \gets \matr{M}^{-1}\vec{z}\) \Comment{Apply preconditioner}
      \State \(\beta \gets \norm{\vec{r}}_2, \quad \vec{v}_1 = \vec{r} / \beta, \quad \matr{V}_1 \gets [\vec{v}_1]\) \Comment{Setup for Arnoldi process}
      \For{\(k = 1, 2, \dots, n_{\text{inner}}\)}
        \State \(\vec{w} \gets \matr{M}^{-1} \matr{A} \vec{v}_k\) \Comment{Apply preconditioner}
        \State \(\beta_1 = \norm{\vec{w}}_2\)
        \State \(\vec{w}, h_{1,k}, \dots, h_{k,k} \gets \text{MGS}(\vec{w}, \matr{V}_k)\) \Comment{Orthogonalization}
        \State \(\beta_2 = \norm{\vec{w}}_2\)
        \If{\(\beta_1 + 0.001 \times \beta_2 = \beta_1\)} \Comment{Brown/Hindmarsh condition \cite{brown_1991_krylov}}
          \State \([\vec{w}, h_{1,k}, \dots, h_{k,k}] \gets [\vec{w}, h_{1,k}, \dots, h_{k,k}] +
          \text{MGS}(\vec{w}, \matr{V}_k)\) \Comment{Reorthogonalize}
        \EndIf
        \State \(h_{k+1,k} \gets \norm{\vec{w}}_2\)
        \State \(\vec{v}_{k+1} \gets \vec{w} / h_{k+1,k}\)
        \State \(\matr{V}_{k+1} \gets [\matr{V}_k, \vec{v}_{k+1}]\) \Comment{Stack basis horizontally}
        \State Apply Givens rotation
        \State Form and store the rotation angle for the next iteration
      \EndFor
      \State \(\matr{H}_k \gets \{h_{i,j}\}_{1 \le i,j \le k}\) \Comment{Form upper
        Hessenberg matrix}
      \State Solve the least squares problem \[\vec{y}_k = \argmin_{\vec{y}\in\mathbb{R}^k} \norm{\beta e_1 - \matr{H}_k \vec{y}}_2\]
      where \(e_1 \in \mathbb{R}^{k + 1}\) is the first standard basis vector \(e_1 = [1, 0, 0, \dots, 0]\transpose\)
      \State \(\vec{x} = \vec{x} + \matr{V}_k \vec{y}_k\) \Comment{Apply correction}
      \State \Return \(\vec{x}\)

      \vspace{10pt}

      \Procedure{MGS}{$\vec{w}, \matr{V}_k$} \Comment{Modified Gram-Schmidt}
        \State \([\vec{v}_1, \dots, \vec{v}_k] \gets \matr{V}_k\)
        \For{\(i = 1, 2, \dots, k\)}
          \State \(h_{i,k} \gets \vec{w} \cdot \vec{v}_i\)
          \State \(\vec{w} \gets \vec{w} - h_{i,k} \vec{v}_i\)
        \EndFor
        \State \Return \(\vec{w}, h_{1,k}, \dots, h_{k,k}\)
      \EndProcedure
    \end{algorithmic}
  \end{algorithm}
\end{singlespace}

\subsection{Fine-Grained Parallel ILU}
\label{sec:fine-grain-parall-1}

Algorithm \ref{algo:fgpilu} follows the fine-grained parallel ILU procedure for
nonsymmetric matrices described in \cite{chow_fine-grained_2015}. Although the
input matrix is assumed to be symmetric, the nonsymmetric ILU variant was
implemented because it proved more robust during development. In particular, the
symmetric ILU formulation occasionally suffered from NaN collapse due to
square-root evaluations of the nonlinear residual when it became negative.

Despite this, the underlying matrix symmetry was still exploited when computing
the unknowns \(u_{i,j}\) and \(l_{i,j}\). Only the upper-triangular entries
\(u_{i,j}\) were explicitly iterated, while the corresponding lower-triangular
entries \(l_{i,j}\) were recovered by symmetry.

To construct the sparsity pattern with a given fill-in level, a
\texttt{std::bitset} was used to efficiently track the nonzero positions.
Because \texttt{std::bitset} requires its size to be fixed at compile time, its
length was chosen to be larger than the problem size \(n\) to ensure coverage
for all rows and columns.

\begin{singlespace}
  \begin{algorithm}[h]
    \caption{Fine-Grained Parallel ILU, given \(\matr{A} \in \mathbb{R}^{n \times n}\) with
      positive diagonal, fill-in level \(k\), maximum number of sweeps
      \(n_{\text{sweep}}\), tolerance \(\epsilon\)}
    \label{algo:fgpilu}
    \begin{algorithmic}[1]
      \State \(\matr{d_{i,i}} \gets 1 / \sqrt{|a_{i,i}|},\quad \matr{D} \gets \{d_{i,i}\}_{1
        \le i \le n}\) \Comment{Diagonal scaling factors of \(\matr{A}\)}
      \State \(\matr{A} \gets \matr{D} \matr{A} \matr{D}\)
      \Comment{Symmetrically scales \(\matr{A}\) such that \(a_{i,i} = 1\)}
      \State \(S \gets \text{sparsity pattern of } \matr{A}^{k+1}\)
      \Comment{Without computing \(\matr{A}^{k+1}\) numerically}
      \State \(\matr{U} \gets \text{upper triangular part of } \matr{A}\)
      \State \(\matr{L} \gets \text{lower triangular part of } \matr{A}
      \text{ and set } l_{ii} = 1\)
      \State \(\beta \gets \text{NLResNorm}()\) \Comment{Computes nonlinear residual norm}
      \For{\(\text{sweep} = 1, 2, \dots n_{\text{sweep}}\)}
        \For{\((i, j) \in S\) \textbf{in parallel}}
          \If{\(i > j\)}
            \State \(l_{i,j} \gets (a_{i,j} - \sum_{k=1}^{j-1} l_{i,k}u_{k,j}) / u_{j,j}\)
          \Else
            \State \(u_{i,j} \gets a_{i,j} - \sum_{k=1}^{i-1} l_{i,k}u_{k,j}\)
          \EndIf
        \EndFor
        \State \(\beta' \gets \text{NLResNorm}()\)
        \If{\(|\beta - \beta'| < \epsilon\)}
          \State break
        \EndIf
        \State \(\beta \gets \beta'\)
      \EndFor

      \vspace{10pt}

      \Procedure{NLResNorm}{}
        \State \Return
        \[
          \sum_{(i,j) \in S} \left| a_{i,j} - \sum_{k=1}^{\min(i,j)} l_{i,k}u_{k,j} \right|
        \]
      \EndProcedure
      \end{algorithmic}
  \end{algorithm}
\end{singlespace}

\subsection{Mixed-Precision IR}
\label{sec:mixed-precision-ir}

Algorithm \ref{algo:ir} outlines the IR procedure in a MP setting: after
factorization, an initial solution is obtained and iteratively refined by
solving preconditioned correction equations. Convergence is monitored through
both update and residual norms, terminating once refinements fall below machine
precision or residual stagnation is detected.

The algorithm distinguishes three precisions: the factorization precision
\(u_f\) for ILU decomposition and triangular solves, the working precision
\(u_w\) for GMRES iterations, and the residual precision \(u_r\) for recomputing
residuals. As mentioned earlier in Section~\ref{sec:c++-templates-mixed}, this
triplet \((u_f, u_w, u_r)\) is enforced through C++ templates and concepts, with
explicit conversion operators to enable exchange between types.

\begin{singlespace}
  \begin{algorithm}[h]
    \caption{GMRES-IR with ILU factorization in MP, given factorization
      precision \(u_f\), working precision \(u_w\), residual precision \(u_r\),
      maximum number of outer iterations \(n_{\text{outer}}\)}
    \label{algo:ir}
    \begin{algorithmic}[1]
      \State Perform ILU factorization of \(\matr{A}\) \Comment{at \(u_f\)}
      \State Solve \(\matr{L}\matr{U} \vec{x}_0 = \vec{b}\) \Comment{at \(u_f\)}
      \State \(\vec{r} \gets \vec{b} - \matr{A}\vec{x}_0\) \Comment{at \(u_r\)}
      \State \(\alpha \gets \infty, \quad \beta \gets \norm{\vec{r}}\)
      \For{\(i \gets 0, \dots, n_{\text{outer}} - 1\) and \(\norm{r_i} \geq \epsilon\)}
        \State \(\gamma \gets \norm{\vec{x}_i}\)
        \State Solve \((\matr{L}\matr{U})^{-1}\matr{A}\vec{d}_i =
        (\matr{L}\matr{U})^{-1}\vec{r_i}\) with preconditioned GMRES \par
        where \(\matr{M}^{-1} = (\matr{L}\matr{U})^{-1}\) \Comment{at \(u_w\)}
        \State \(\vec{x}_{i+1} = \vec{x}_i + \vec{d}_i\) \Comment{at \(u_w\)}
        \State \(\vec{r}_i \gets \vec{b} - \matr{A}\vec{x}_i\) \Comment{at \(u_r\)}
        \State \(\alpha' \gets \norm{\vec{d}_i}, \quad \beta' \gets \norm{\vec{r}_i}\)
        \If{\(\alpha' \le \gamma \times (10 \times \epsilon_{\text{mach}}(u_w))\) or \(|\beta' - \beta| < \epsilon\)}
          \State break
        \EndIf
        \State \(\alpha \gets \alpha', \quad \beta \gets \beta'\)
      \EndFor
      \State \Return \(\vec{x}_{i+1}\)
    \end{algorithmic}
  \end{algorithm}
\end{singlespace}
