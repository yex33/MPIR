\chapter{Literature Review}
\label{cha:literature-review}

This chapter provides an overview of existing research and foundational concepts
related to improving the performance of the GMRES method using mixed-precision
techniques. It covers iterative solvers in the context of mixed-precision
computing as well as several preconditioning strategies.

\section{Iterative Solvers and MP Computing}
\label{sec:iterative-solvers}

The GMRES method, introduced by \textcite{saad_gmres_1986} in
\citeyear{saad_gmres_1986}, is a widely used iterative Krylov subspace method
for solving sparse, non-symmetric systems of linear equations arising from many
scientific applications. As a Krylov subspace method, GMRES constructs an
orthogonal basis using Arnoldi’s procedure and then finds a solution vector
within that subspace that minimizes the resulting residual.

A significant extension to GMRES is the introduction of restarting, in which the
solver computes the current solution vector after a certain number of iterations
and then restarts with an empty Krylov subspace, using the newly computed
solution as the initial guess \cite{lindquist_improving_2020}. This technique is
essential for limiting the number of basis vectors in the Krylov subspace,
thereby reducing storage requirements and computation costs associated with
orthogonalizing new vectors. Notably, restarted GMRES is functionally equivalent
to iterative refinement, in which a non-restarted GMRES computes the error
correction \cite{lindquist_improving_2020,mary_mixed_2023}. We will revisit this
equivalence relationship later in Section~\ref{sec:restarting} in a formal
context.

Iterative refinement (IR) is an established technique designed to improve the
accuracy of a computed solution to a linear system by iteratively computing and
correcting errors based on the system's residuals.
\textcite{moler_iterative_1967} advanced the understanding of IR in the context
of floating-point arithmetic. He highlighted that iterative refinement is
effective in reducing roundoff errors and noted that computing the residuals
requires higher precision arithmetic to achieve accurate final results.

Another major advancement in IR was the adoption of mixed-precision (MP)
arithmetic. This approach typically involves performing computationally
intensive tasks, such as matrix factorization, in lower precision to gain speed
and memory efficiency, while accuracy-critical operations such as residual
computation are performed in higher precision to maintain overall numerical
stability and accuracy. As reported by \textcite{wong_exploring_2024}, early
applications of MP IR used single precision for LU factorization and double
precision for residual and correction updates, thereby improving computational
efficiency while preserving numerical stability. This concept was later extended
to preconditioned GMRES within the MP iterative refinement framework, where
correction updates are computed using the GMRES method in double precision.
\textcite{lindquist_improving_2020} have shown that this approach is
particularly effective for ill-conditioned linear systems.

\section{Preconditioning}
\label{sec:preconditioning}

Preconditioning is a technique used to improve the convergence behavior of
iterative linear system solvers. It works by transforming the original linear
system into an equivalent one with more favorable spectrum—--typically a smaller
spectral condition number or eigenvalues clustered closer to one
\cite[p.~187]{ascher_first_2011}. The preconditioning matrix, \(\matr{M}\),
serves as an approximation to the original matrix \(\matr{A}\). Common
preconditioning strategies include left preconditioning (\(\matr{M}^{-1}
\matr{A} \vec{x} = \matr{M}^{-1} \vec{b}\)) and right preconditioning
(\(\matr{A} \matr{M}^{-1} \vec{y} = \vec{b}, \vec{x} = \matr{M}^{-1} \vec{y}\)).

Standard preconditioners include the LU factors of \(\matr{A}\) (\(\matr{A} =
\matr{L} \matr{U}\)). A related approach for symmetric positive definite (SPD)
matrices is Cholesky factorization (\(\matr{A} = \matr{U} \matr{U}\transpose{}\)
or \(\matr{A} = \matr{L} \matr{D} \matr{L}\transpose{}\) for symmetric
matrices). The QDLDL package \cite{stellato_osqp_2020} computes the
LDL\textsuperscript{T} factorization of a quasi-definite matrix.
\textcite{shahrooz_derakhshan_using_2023} adopted it for mixed precision by
templating it on the floating-point data type. These are symmetric matrices of a
specific block form guaranteed to have an LDL\textsuperscript{T} factorization.
\textcite{wong_exploring_2024}'s work establishes a concrete C++ implementation
of a preconditioned GMRES-IR solver that supports MP arithmetic. This solver
leverages the QDLDL package for computing the LDL\textsuperscript{T}
factorization of quasi-definite matrices as the preconditioners.

When solving large sparse linear systems, direct methods such as LU and
LDL\textsuperscript{T} factorization usually suffer from substantial fill-in,
making them computationally expensive in both memory and time. As shown in
\cite{wong_exploring_2024}, factorization time is a dominant component of the
total runtime for large sparse matrices where LDL\textsuperscript{T}
factorization is used. Since preconditioning requires only an approximation that
accelerates convergence, rather than an exact factorization, incomplete
factorizations such as incomplete LU (ILU) (\(\matr{A} \approx
\widetilde{\matr{L}} \widetilde{\matr{U}}\)) and incomplete Cholesky (IC)
(\(\matr{A} \approx \widetilde{\matr{U}} \widetilde{\matr{U}}\transpose\)) are
commonly employed.

A fine-grained parallel ILU algorithm, proposed by
\textcite{chow_fine-grained_2015}, offers a distinct approach to computing ILU
factorizations in parallel. Their method reformulates ILU factorization as the
solution of a system of bilinear equations \((\matr{L} \matr{U})_{i,j} =
a_{i,j}\) for entries in a specified sparsity pattern. This system of unknowns
can be solved in parallel using fixed-point iteration sweeps, where each
component of the new iterate is computed asynchronously, often using OpenMP. A
symmetric variant is available for IC factorization. A key finding is that only
a few sweeps are needed to construct an effective preconditioner.

