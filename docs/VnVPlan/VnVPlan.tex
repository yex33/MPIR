\documentclass[12pt, titlepage]{article}

\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage{xr,xr-hyper}
\externaldocument{../SRS/SRS}

\usepackage[backend=biber,style=authoryear]{biblatex}
\addbibresource{../../refs/References.bib}

% Commands for survey template
% https://github.com/annerosenisser/latex-surveys
\usepackage{wasysym}  % provides \ocircle and \Box
\usepackage{enumitem} % easy control of topsep and leftmargin for lists
\usepackage{color}    % used for background color
\usepackage{forloop}  % used for \Qrating and \Qlines
\usepackage{ifthen}   % used for \Qitem and \QItem

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Beginning of questionnaire command definitions %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% 2010, 2012 by Sven Hartenstein
%% mail@svenhartenstein.de
%% http://www.svenhartenstein.de
%%
%% Please be warned that this is NOT a full-featured framework for
%% creating (all sorts of) questionnaires. Rather, it is a small
%% collection of LaTeX commands that I found useful when creating a
%% questionnaire. Feel free to copy and adjust any parts you like.
%% Most probably, you will want to change the commands, so that they
%% fit your taste.
%%
%% Also note that I am not a LaTeX expert! Things can very likely be
%% done much more elegant than I was able to. If you have suggestions
%% about what can be improved please send me an email. I intend to
%% add good tipps to my website and to name contributers of course.
%%
%% 10/2012: Thanks to karathan for the suggestion to put \noindent
%% before \rule!

%% \Qq = Questionaire question. Oh, this is just too simple. It helps
%% making it easy to globally change the appearance of questions.
\newcommand{\Qq}[1]{\textbf{#1}}

%% \QO = Circle or box to be ticked. Used both by direct call and by
%% \Qrating and \Qlist.
\newcommand{\QO}{$\Box$}% or: $\ocircle$

%% \Qrating = Automatically create a rating scale with NUM steps, like
%% this: 0--0--0--0--0.
\newcounter{qr}
\newcommand{\Qrating}[1]{\QO\forloop{qr}{1}{\value{qr} < #1}{---\QO}}

%% \Qline = Again, this is very simple. It helps setting the line
%% thickness globally. Used both by direct call and by \Qlines.
\newcommand{\Qline}[1]{\noindent\rule{#1}{0.6pt}}

%% \Qlines = Insert NUM lines with width=\linewith. You can change the
%% \vskip value to adjust the spacing.
\newcounter{ql}
\newcommand{\Qlines}[1]{\forloop{ql}{0}{\value{ql}<#1}{\vskip0em\Qline{\linewidth}}}

%% \Qlist = This is an environment very similar to itemize but with
%% \QO in front of each list item. Useful for classical multiple
%% choice. Change leftmargin and topsep accourding to your taste.
\newenvironment{Qlist}{%
\renewcommand{\labelitemi}{\QO}
\begin{itemize}[leftmargin=1.5em,topsep=-.5em]
}{%
\end{itemize}
}

%% \Qtab = A "tabulator simulation". The first argument is the
%% distance from the left margin. The second argument is content which
%% is indented within the current row.
\newlength{\qt}
\newcommand{\Qtab}[2]{
\setlength{\qt}{\linewidth}
\addtolength{\qt}{-#1}
\hfill\parbox[t]{\qt}{\raggedright #2}
}

%% \Qitem = Item with automatic numbering. The first optional argument
%% can be used to create sub-items like 2a, 2b, 2c, ... The item
%% number is increased if the first argument is omitted or equals 'a'.
%% You will have to adjust this if you prefer a different numbering
%% scheme. Adjust topsep and leftmargin as needed.
\newcounter{itemnummer}
\newcommand{\Qitem}[2][]{% #1 optional, #2 notwendig
\ifthenelse{\equal{#1}{}}{\stepcounter{itemnummer}}{}
\ifthenelse{\equal{#1}{a}}{\stepcounter{itemnummer}}{}
\begin{enumerate}[topsep=2pt,leftmargin=2.8em]
\item[\textbf{\arabic{itemnummer}#1.}] #2
\end{enumerate}
}

%% \QItem = Like \Qitem but with alternating background color. This
%% might be error prone as I hard-coded some lengths (-5.25pt and
%% -3pt)! I do not yet understand why I need them.
\definecolor{bgodd}{rgb}{0.8,0.8,0.8}
\definecolor{bgeven}{rgb}{0.9,0.9,0.9}
\newcounter{itemoddeven}
\newlength{\gb}
\newcommand{\QItem}[2][]{% #1 optional, #2 notwendig
\setlength{\gb}{\linewidth}
\addtolength{\gb}{-5.25pt}
\ifthenelse{\equal{\value{itemoddeven}}{0}}{%
\noindent\colorbox{bgeven}{\hskip-3pt\begin{minipage}{\gb}\Qitem[#1]{#2}\end{minipage}}%
\stepcounter{itemoddeven}%
}{%
\noindent\colorbox{bgodd}{\hskip-3pt\begin{minipage}{\gb}\Qitem[#1]{#2}\end{minipage}}%
\setcounter{itemoddeven}{0}%
}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% End of questionnaire command definitions %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{../Comments}
\input{../Common}

\newcounter{testnum} % Test Number
\newcommand{\tthetestnum}{GD\thetestnum}
\newcommand{\tref}[1]{T\ref{#1}}

\newcommand{\rref}[1]{R\ref{#1}}
\newcommand{\nfrref}[1]{NFR\ref{#1}}

\begin{document}

\title{System Verification and Validation Plan for \progname{}}
\author{\authname}
\date{\today}

\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{4cm}p{2cm}X}
  \toprule {\bf Date} & {\bf Version} & {\bf Notes}   \\
  \midrule
  24 February 2025    & 1.0           & Initial draft \\
  \bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l}
  \toprule
  \textbf{symbol} & \textbf{description}                     \\
  \midrule
  API       & Application Programming Interface  \\
  CI        & Continuous Integration             \\
  MG        & Module Guide                       \\
  MIS       & Module Interface Specification     \\
  SRS       & Software Requirement Specification \\
  T         & Test                               \\
  VnV       & Verification and Validation        \\
  \bottomrule
\end{tabular}

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \cite{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document provides an introductory blurb and roadmap of the Verification and
Validation (VnV) plan

\section{General Information}

This section provides a brief description of the project background and
introduces the objectives for the VnV plan.

\subsection{Summary}

\wss{Say what software is being tested.  Give its name and a brief overview of
  its general functions.}

\progname{} is a sparse linear solver designed to solve large, sparse real
matrices efficiently. It uses the General Minimal Residual (GMRES) method for
internal matrix solves and iterative refinement techniques to improve both speed
and accuracy. The software is intended for use in computational science,
engineering, and numerical analysis applications. As a complete library suite,
the software also includes example programs to demonstrate the solver interfaces
and practical use cases of the solver.

\subsection{Objectives}

\wss{State what is intended to be accomplished.  The objective will be around
  the qualities that are most important for your project.  You might have
  something like: ``build confidence in the software correctness,''
  ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
  just those that are most important.}

\wss{You should also list the objectives that are out of scope. You don't have
  the resources to do everything, so what will you be leaving out. For instance,
  if you are not going to verify the quality of usability, state this. It is
  also worthwhile to justify why the objectives are left out.}

\wss{The objectives are important because they highlight that you are aware of
  limitations in your resources for verification and validation. You can't do
  everything, so what are you going to prioritize? As an example, if your system
  depends on an external library, you can explicitly state that you will assume
  that external library has already been verified by its implementation team.}

The primary objective of the Verification and Validation (VnV) plan is to ensure
the correctness, accuracy, and efficiency of \progname{} in solving sparse linear
systems. The secondary objective is to verify usability and maintainability of the
software for integration with other numerical libraries.

Usability testing for non-expert users is not prioritized is out of the scope of
this VnV plan, as \progname{} is only intended for domain-expert users. The
solver is expected to use an external library for matrix factorization. The
example programs will also depend on an external library for reading and writing
sparse matrices in Matrix Market Exchange Format (\cite{noauthor_matrix_2013}).

\subsection{Challenge Level and Extras}

\wss{State the challenge level (advanced, general, basic) for your project.
Your challenge level should exactly match what is included in your problem
statement.  This should be the challenge level agreed on between you and the
course instructor.  You can use a pull request to update your challenge level
(in TeamComposition.csv or Repos.csv) if your plan changes as a result of the
VnV planning exercise.}

\wss{Summarize the extras (if any) that were tackled by this project.  Extras
can include usability testing, code walkthroughs, user documentation, formal
proof, GenderMag personas, Design Thinking, etc.  Extras should have already
been approved by the course instructor as included in your problem statement.
You can use a pull request to update your extras (in TeamComposition.csv or
Repos.csv) if your plan changes as a result of the VnV planning exercise.}

The challenge level remains the same at the general level. The extra is expected
to be conducting a usability test which will be discussed in
Section~\ref{sec:usability}.

\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS
  and your other project documents (design documents, like MG, MIS, etc).  You
  can include these even before they are written, since by the time the project
  is done, they will be written.  You can create BibTeX entries for your
  documents and within those entries include a hyperlink to the documents.}

See the Software Requirements Specification (\cite{SRS}) for \progname{}, which
details the goals, requirements, assumptions, and theories of the software. The
Module Guide (\cite{MG}) and Module Interface Specification (\cite{MIS}) document the
design of \progname{}.

\section{Plan}

This section details the plan for the verification of both the documents and the
software for \progname{}. The primary artifacts being verified are: SRS,
software design, VnV Plan, and implementation.

\subsection{Verification and Validation Team}

The table below summarizes the VnV Team for the project:

\vspace{5pt}

\begin{tabularx}{\linewidth}{lX}
  \toprule
  \textbf{Team Member} & \textbf{Role}                           \\
  \midrule
  Xunzhou Ye     & Lead developer and tester         \\
  Qianlin Chen   & ``Domain expert'', provides feedbacks on documents per course
                   guidelines and document templates \\
  Dr. Nedialkov  & Primary stakeholder, oversees project direction and validates
                   all documents                     \\
  \bottomrule
\end{tabularx}

% machine epsilon: the smallest difference in value that the computer can tell
% apart at that given precision

% dependent external library for benchmarking
% google-benchmark for micro-benchmarking, repeats benchmark cases until a
% statistically stable result is obtained.

% why choose runtime?
% The solver is designed to trade computation complexity for space complexity
% Cannot use common complexity analysis, big-O notation to qualify the
% performance of the software.

% Choosing runtime as the only performance metric, open to ideas
% runtime results are unstable because:
% 1. CPU clock frequency, temperature
% 2. OS scheduling, interrupts, context switches
% 3. hardware level optimizations of certain arithmetics at certain precision,
% CPU vs. GPU
% 4. high speed caching
% 5. Inconsistency between runs, hard to reproduce results

\subsection{SRS Verification Plan}

The SRS will be verified via feedbacks from the assigned domain expert and the
project supervisor. A SRS checklist (\cite{SRS_checklist}) will be used to guide
the review process. After each major revision of the SRS, a meeting with the
project supervisor shall be scheduled within two weeks of the revision date.
Changes to the SRS shall be presented and discussed with the supervisor. All
feedbacks from both the domain expert and the supervisor will be documented in
the form of GitHub issues assigned to the project owner. The project owner is
responsible to address and close these issues as they iterate and revise the SRS
document. The key objective is to verify that the software requirements and the
documentation are sound and coherent to the intended users and defined in the
SRS.

\subsection{Design Verification Plan}

Since the software design is derived from a research study, the underlying
algorithms and mathematical models are predefined. As a result, the scope for
structural modifications is limited, and design decisions must align with the
established research framework. The primary focus is on enhancing performance,
ensuring software quality, and maintaining consistency with the research
objectives.

The VnV team will conduct structured reviews of the design, leveraging their
professional expertise to provide informed feedback. This evaluation will be
guided by the MG and MIS checklists (\cite{MG_checklist, MIS_checklist}) to ensure
that design principles are adhered to and best practices are followed. Similar
to the SRS verification process, all feedback will be documented as GitHub
issues to maintain transparency and traceability.

\subsection{Verification and Validation Plan Verification Plan}

The VnV checklist (\cite{VnV_checklist}) will be used to review each iteration of
the VnV Plan. The goal is to uncover any mistakes and reveal any coverage gaps
through the supervision and review of the VnV team members. Once the project
reaches a deliverable milestone, the VnV team will check whether the documented
testing plans and verification processes have been accomplished and the
requirements fulfilled. Feedbacks will again be documented as GitHub issues.

\subsection{Implementation Verification Plan}

Both automated and manual testing will be performed for this project. For
automated static code analysis, linters will be integrated as part of the
Continuous Integration (CI) pipeline via GitHub Actions. Details on the choice
of tools and its use in the project will be discussed in
Section~\ref{sec:autom-test-verif} below. Plans and schemes for automated
dynamic tests will be done at various levels of abstraction, including unit
tests, system tests. Details are listed in Section~\ref{sec:system-tests} and
\ref{sec:unit-test-descr}. A detailed code walkthrough on core algorithms will
be conducted with the project supervisor to ensure that the implementation align
with the established research work that this project is based on.

\subsection{Automated Testing and Verification Tools}
\label{sec:autom-test-verif}

The software is expected to be implemented in C++. The following are the
language specific tools that will be used for automated testing and
verifications:

\begin{itemize}
\item CMake (\cite{noauthor_cmake_nodate}) will be used to streamline the building and
  testing process of the software. CTest, which is part of CMake, will be used
  to generate code coverage reports for unit tests.
\item doctest (\cite{noauthor_doctestdoctest_2025}) will be used as the unit testing
  framework for writing test cases.
\item clang-format (\cite{noauthor_clangformat_nodate}) will be used to format source
  codes based on a set of predefined rules specified in a configuration file. A
  \href{https://git-scm.com/book/ms/v2/Customizing-Git-Git-Hooks}{Git Hook} will
  be deployed to run a format check to ensure that all committed source codes
  are properly formatted. The goal of formatting codes is to improve code
  readability and consistency.
\item clang-tidy (\cite{noauthor_clang-tidy_nodate}) will be used as the linter for
  static code analysis. Committed codes should be free of linter errors and
  warnings to minimize the chance of having incorrect codes.
\item Benchmark (\cite{noauthor_googlebenchmark_2025}) will be used to evaluate the
  runtime performance of the solver as part of the nonfunctional verification
  process.
\end{itemize}

Apart from these language specific tools, the following general purposed tools
are also used:

\begin{itemize}
\item GitHub Actions will be used as the CI pipeline for the project. Most of the
  language specific automated tools mentioned above will be integrated as part
  of CI to ensure that the verification process is reproducible in an isolated,
  remote environment. This verifies the verification process itself and further
  improves confidence of the verification process.
\end{itemize}

\subsection{Software Validation Plan}

A valid linear solver is one that correctly solves linear systems. In the
context of this project, the validity of the software is primarily determined by
its correctness---ensuring that the computed solutions satisfy the given
equations within an acceptable tolerance. Since this software is part of a
research study, its validity is also assessed by how well the implementation
adheres to the established specifications and aligns with prior research work.
To verify this, code walkthroughs of the core algorithms will be conducted with
the project supervisor.

Beyond correctness, performance also plays a role in validation. The solver is
designed to offer advantages over existing solutions, making performance
benchmarking an essential validation step. To assess this, an automated
performance benchmark will be conducted, generating empirical runtime data for
different problem sizes. These results will be manually compared against
established solvers to evaluate the solver’s computational efficiency.

\section{System Tests}
\label{sec:system-tests}

This section outlines the tests that will be performed for \progname{} to verify
both the functional and nonfunctional requirements specified in the SRS
(\cite{SRS}). Input specifications and constraints are also listed in the SRS.

\subsection{Tests for Functional Requirements}
\label{sec:tests-funct-requ}

In this section, the system tests that will be conducted are described in
detail. These tests will be used to verify the fulfillment of the functional
requirements as listed in the SRS (\cite{SRS}).

\subsubsection{Matrix Inputs and Outputs}

This section covers the requirement \rref{R:ex} of the SRS. This includes
essentially a ``driver'' for the solver which loads sparse matrices from a text
file in Matrix Market Exchange (.mtx) Format (\cite{noauthor_matrix_2013}) into memory,
invokes the solver interfaces, and outputs the results returned from the solver.
The tests described below will verify that such ``driver'' is functional.

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:io}:]{matrix-io}

Control: Automatic

Initial State: A hard coded matrix \(\matr{A}\) of size \(\num{100} \times
\num{100}\) in the programming language of choice is instantiated.

Input: matrix \(\matr{A}\) in plain text .mtx format of size \(\num{100} \times
\num{100}\), a random vector \(\vec{b}\) of size \num{100}. \(u_f = u_w = u_r =
\texttt{double}\)

Output: The elements of \(\matr{A}\) matches the hard-coded one. \(\vec{x}\) of size
\num{100}

Test Case Derivation: N/A

How test will be performed: Automatic

\end{itemize}

\subsubsection{Correctness Tests with Manufactured Solutions}

This section covers one of the ways to verify the requirements \rref{R:Axb} and
\rref{R:MP} of the SRS. This includes tests on the accuracy of the yielded
solution from the solver by manufacturing an exact solution \(\vec{x}_\mathrm{ref}\) to the
problem \(\matr{A}\vec{x} = \vec{b}\). This manufacturing process loosely follows the scheme
below:

\begin{enumerate}
\item \(\vec{x}_\mathrm{ref} \gets \text{some random vector}\)
\item \(\vec{b} \gets \matr{A} \vec{x}_\mathrm{ref} \)
\item Solve \(\matr{A}\vec{x} = \vec{b}\)
\item \(e \gets \displaystyle \frac{\norm{\vec{x} - \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2}\)
\end{enumerate}

The relative error \(e\) will be used as the accuracy metric.

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:gdd}:]{generated-double-double}

Control: Automatic

Initial State: matrix \(\matr{A}\) is read from file and stored in memory.
An expected exact solution \(\vec{x}_\mathrm{ref}\) is prepared.

Input: matrix \(\matr{A}\) of size \(\num{10000} \times \num{10000}\) with
\(\cond{\matr{A}} \approx \num{1e2}\), \(\vec{b}\) of size \num{10000}, \(u_f =
u_w = u_r = \texttt{double}\)

Output: \(\vec{x}\) of size \num{10000} such that \(e = \displaystyle \frac{\norm{\vec{x} -
    \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2} < \num{1e-12}\)

Test Case Derivation: \(\vec{x}_\mathrm{ref}\) is randomly generated. \(\vec{b} = \matr{A}\vec{x}_\mathrm{ref}\)

How test will be performed: Automatic

\item[T\refstepcounter{testnum}\thetestnum \label{T:gsd}:]{generated-single-double}

Control: Automatic

Initial State: matrix \(\matr{A}\) is read from file and stored in memory.
An expected exact solution \(\vec{x}_\mathrm{ref}\) is prepared.

Input: matrix \(\matr{A}\) of size \(\num{10000} \times \num{10000}\) with
\(\cond{\matr{A}} \approx \num{1e2}\), \(\vec{b}\) of size \num{10000}, \(u_f = \texttt{single}, u_w
= u_r = \texttt{double}\)

Output: \(\vec{x}\) of size \num{10000} such that \(e = \displaystyle \frac{\norm{\vec{x} -
    \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2} < \num{1e-12}\)

Test Case Derivation: \(\vec{x}_\mathrm{ref}\) is randomly generated. \(\vec{b} =
\matr{A}\vec{x}_\mathrm{ref}\). Matrix factorization is done in single precision while
working and residual computing are done in double precision.

How test will be performed: Automatic

\end{itemize}

\subsubsection{Correctness Tests against Trusted Solvers}

This section covers the other way to verify the requirements \rref{R:Axb} and
\rref{R:MP} of the SRS. This includes tests on the accuracy of the yielded
solution from the solver by comparing it to an external, trusted solver to the
problem \(\matr{A}\vec{x} = \vec{b}\). This process loosely follows the scheme
below:

\begin{enumerate}
\item \(\vec{x}_\mathrm{ref} \gets \text{solution by an external solver}\)
\item Solve \(\matr{A}\vec{x} = \vec{b}\)
\item \(e \gets \displaystyle \frac{\norm{\vec{x} - \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2}\)
\end{enumerate}

The relative error \(e\) will be used as the accuracy metric.

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:exdd}:]{external-double-double}

Control: Automatic

Initial State: matrix \(\matr{A}\) is read from file and stored in memory. The
same problem \(\matr{A}\vec{x} = \vec{b}\) is passed to the external solver. A
reference solution \(\vec{x}_\mathrm{ref}\) is prepared.

Input: matrix \(\matr{A}\) of size \(\num{10000} \times \num{10000}\) with
\(\cond{\matr{A}} \approx \num{1e2}\), \(\vec{b}\) of size \num{10000}, \(u_f =
u_w = u_r = \texttt{double}\)

Output: \(\vec{x}\) of size \num{10000} such that \(e = \displaystyle \frac{\norm{\vec{x} -
    \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2} < \num{1e-12}\)

Test Case Derivation: \(\vec{x}_\mathrm{ref}\) is randomly generated. \(\vec{b} = \matr{A}\vec{x}_\mathrm{ref}\)

How test will be performed: Automatic

\item[T\refstepcounter{testnum}\thetestnum \label{T:exsd}:]{external-single-double}

Control: Automatic

Initial State: matrix \(\matr{A}\) is read from file and stored in memory. The
same problem \(\matr{A}\vec{x} = \vec{b}\) is passed to the external solver. A
reference solution \(\vec{x}_\mathrm{ref}\) is prepared.

Input: matrix \(\matr{A}\) of size \(\num{10000} \times \num{10000}\) with
\(\cond{\matr{A}} \approx \num{1e2}\), \(\vec{b}\) of size \num{10000}, \(u_f = \texttt{single}, u_w
= u_r = \texttt{double}\)

Output: \(\vec{x}\) of size \num{10000} such that \(e = \displaystyle \frac{\norm{\vec{x} -
    \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2} < \num{1e-12}\)

Test Case Derivation: \(\vec{x}_\mathrm{ref}\) is randomly generated. \(\vec{b} =
\matr{A}\vec{x}_\mathrm{ref}\). Matrix factorization is done in single precision while
working and residual computing are done in double precision.

How test will be performed: Automatic

\end{itemize}

\subsection{Tests for Nonfunctional Requirements}

\wss{For some nonfunctional tests, you won't be setting a target threshold for
passing the test, but rather describing the experiment you will do to measure
the quality for different inputs.  For instance, you could measure speed versus
the problem size.  The output of the test isn't pass/fail, but rather a summary
table or graph.}

\wss{Tests related to usability could include conducting a usability test and
  survey.  The survey will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the
format for the tests given below.}

\wss{If you introduce static tests in your plan, you need to provide details.
How will they be done?  In cases like code (or document) walkthroughs, who will
be involved? Be specific.}

In this section, the system tests that will be conducted are described in
detail. These tests will be used to verify the fulfillment of the nonfunctional
requirements as listed in the SRS (\cite{SRS}).

\subsubsection{Accuracy}

The accuracy of the solver will be assessed by verifying that it converges to a
solution within the user-defined tolerance \(\epsilon\). The level of accuracy
required for computational science and engineering applications will be
evaluated through the relative residual norm after convergence. The following
tests will be performed to verify the nonfunctional requirement \nfrref{NFR:acc}
in the SRS:

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:acc}:]{nfr-acc}

Type: Functional, Automated

Initial State: Similar to \tref{T:gdd}, matrix \(\matr{A}\) is read from file
and stored in memory. An manufactured expected exact solution
\(\vec{x}_\mathrm{ref}\) is prepared.

Input: Similar to \tref{T:gdd}, with an extra input for user-specified tolerance
\(\epsilon = \num{1e-12}\).

Output/Result: \(\vec{x}\) of size \num{10000} such that \(e = \displaystyle
\frac{\norm{\vec{x} - \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2} <
\epsilon = \num{1e-12}\). The computed solution’s residual norm should be below
the threshold \(\epsilon\). If convergence is not achieved after iterations, a
warning should be issued.

How test will be performed: Automatic

\end{itemize}

\subsubsection{Usability}
\label{sec:usability}

The usability of the solver will be evaluated based on the clarity and
accessibility of its public Application Programming Interface (API). The API
should be self-contained, readable, and easy to integrate into other software as
a dependency. Usability testing will reference the user characteristics section
and include developer feedback. The following tests will be performed to verify
the nonfunctional requirement \nfrref{NFR:use} in the SRS:

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:use}:]{nfr-use}

Type: Static, Review, Survey-Based

Initial State: The API documentation and usage examples are available. The API
documentation of an external, established sparse linear solver is also presented
as a comparable reference.

Input: VnV team members review the API and compare it against that of the
external solver.

Output/Result: Feedback on clarity, ease of integration, pros and cons, and
suggestions for improvements.

How test will be performed: A usability survey will be conducted (see
Appendix~\ref{sec:usab-surv-quest}). API walkthroughs and documentation inspections
will be used to ensure that function calls and configurations are intuitive.

\end{itemize}

\subsubsection{Maintainability}

The effort required to modify the solver should be kept minimal. This will be
verified by estimating the complexity of implementing likely changes and
ensuring that modifications take less than a fraction of the original
development time. The following tests will be performed to verify the
nonfunctional requirement \nfrref{NFR:mt} in the SRS:

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:mt}:]{nfr-mt}

Type: Static, Code Review

Initial State: The codebase is available and structured.

Input: Introduce a minor algorithmic modification or API extension.

Output/Result: Measure the number of files and lines of code modified, as well
as time taken.

How test will be performed: A code walkthrough will be conducted with team
members to assess maintainability using the MG and MIS guidelines.


\item[T\refstepcounter{testnum}\thetestnum \label{T:fmt}:]{nfr-format}

Type: Static, Automated

Initial State: The source code is available in a repository with predefined
formatting and linting rules.

Input: Run automated checks using clang-format for code style enforcement and
clang-tidy for static analysis.

Output/Result: The code should conform to the predefined formatting rules, and
no critical linting warnings or errors should remain unaddressed.

How test will be performed: As part of the CI pipeline, clang-format and
clang-tidy will be executed on every commit. Any violations must be resolved
before merging changes.

\end{itemize}

\subsubsection{Portability}

The solver should run on all actively maintained operating systems, including
Windows 10, Windows 11, Linux, and MacOS. Compatibility testing will verify that
all required functionalities work across different platforms. The following
tests will be performed to verify the nonfunctional requirement \nfrref{NFR:port}
in the SRS:

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:port}:]{nfr-port}

Type: Functional, Automated

Initial State: The solver is compiled and deployed on different machines with
different operating systems.

Input: Run both the example programs and all automated functional tests
described in Section~\ref{sec:tests-funct-requ}.

Output/Result: The solver successfully compiles, executes test cases, and
produces correct results.

How test will be performed: Automated CI pipelines will test the software across
different platforms. Success will be determined by running the test suite in
each environment and confirming consistent results.

\end{itemize}

\subsection{Traceability Between Test Cases and Requirements}

% \tref{T:io}
% \tref{T:gdd}
% \tref{T:gsd}
% \tref{T:exdd}
% \tref{T:exsd}
% \tref{T:acc}
% \tref{T:use}
% \tref{T:mt}
% \tref{T:fmt}
% \tref{T:port}
% \rref{R:Axb}
% \rref{R:MP}
% \rref{R:ex}
% \nfrref{NFR:acc}
% \nfrref{NFR:use}
% \nfrref{NFR:mt}
% \nfrref{NFR:port}

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|}                                      \hline
                  & \rref{R:Axb} & \rref{R:MP} & \rref{R:ex} & \nfrref{NFR:acc} & \nfrref{NFR:use} & \nfrref{NFR:mt} & \nfrref{NFR:port} \\ \hline
    \tref{T:io}   &              &             & X           &                  &                  &                 &                   \\ \hline
    \tref{T:gdd}  & X            & X           &             &                  &                  &                 &                   \\ \hline
    \tref{T:gsd}  & X            & X           &             &                  &                  &                 &                   \\ \hline
    \tref{T:exdd} & X            & X           &             &                  &                  &                 &                   \\ \hline
    \tref{T:exsd} & X            & X           &             &                  &                  &                 &                   \\ \hline
    \tref{T:acc}  &              &             &             & X                &                  &                 &                   \\ \hline
    \tref{T:use}  &              &             &             &                  & X                &                 &                   \\ \hline
    \tref{T:mt}   &              &             &             &                  &                  & X               &                   \\ \hline
    \tref{T:fmt}  &              &             &             &                  &                  & X               &                   \\ \hline
    \tref{T:port} &              &             &             &                  &                  &                 & X                 \\ \hline
  \end{tabular}
  \caption{Traceability matrix showing the connections between test cases and
    requirements}
  \label{Table:T_trace}
\end{table}

\section{Unit Test Description}
\label{sec:unit-test-descr}

This section should not be filled in until after the MIS (detailed design
  document) has been completed.

% \wss{Reference your MIS (detailed design document) and explain your overall
% philosophy for test case selection.}

% \wss{To save space and time, it may be an option to provide less detail in this section.
% For the unit tests you can potentially layout your testing strategy here.  That is, you
% can explain how tests will be selected for each module.  For instance, your test building
% approach could be test cases for each access program, including one test for normal behaviour
% and as many tests as needed for edge cases.  Rather than create the details of the input
% and output here, you could point to the unit testing code.  For this to work, you code
% needs to be well-documented, with meaningful names for all of the tests.}

% \subsection{Unit Testing Scope}

% \wss{What modules are outside of the scope.  If there are modules that are
%   developed by someone else, then you would say here if you aren't planning on
%   verifying them.  There may also be modules that are part of your software, but
%   have a lower priority for verification than others.  If this is the case,
%   explain your rationale for the ranking of module importance.}

% \subsection{Tests for Functional Requirements}

% \wss{Most of the verification will be through automated unit testing.  If
%   appropriate specific modules can be verified by a non-testing based
%   technique.  That can also be documented in this section.}

% \subsubsection{Module 1}

% \wss{Include a blurb here to explain why the subsections below cover the module.
%   References to the MIS would be good.  You will want tests from a black box
%   perspective and from a white box perspective.  Explain to the reader how the
%   tests were selected.}

% \begin{enumerate}

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}

% Initial State:

% Input:

% Output: \wss{The expected result for the given inputs}

% Test Case Derivation: \wss{Justify the expected value given in the Output field}

% How test will be performed:

% \item{test-id2\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}

% Initial State:

% Input:

% Output: \wss{The expected result for the given inputs}

% Test Case Derivation: \wss{Justify the expected value given in the Output field}

% How test will be performed:

% \item{...\\}

% \end{enumerate}

% \subsubsection{Module 2}

% ...

% \subsection{Tests for Nonfunctional Requirements}

% \wss{If there is a module that needs to be independently assessed for
%   performance, those test cases can go here.  In some projects, planning for
%   nonfunctional tests of units will not be that relevant.}

% \wss{These tests may involve collecting performance data from previously
%   mentioned functional tests.}

% \subsubsection{Module ?}

% \begin{enumerate}

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}

% Initial State:

% Input/Condition:

% Output/Result:

% How test will be performed:

% \item{test-id2\\}

% Type: Functional, Dynamic, Manual, Static etc.

% Initial State:

% Input:

% Output:

% How test will be performed:

% \end{enumerate}

% \subsubsection{Module ?}

% ...

% \subsection{Traceability Between Test Cases and Modules}

% \wss{Provide evidence that all of the modules have been considered.}

\newpage

\printbibliography{}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions}
\label{sec:usab-surv-quest}

\Qitem{
  \Qq{Did you find the function names and parameters intuitive?}
  \begin{Qlist}
  \item Very intuitive
  \item Somewhat intuitive
  \item Neutral
  \item Confusing
  \item Very confusing
  \end{Qlist}
}

\Qitem{
  \Qq{Did the provided documentation clearly explain how to use the API?}
  \begin{Qlist}
    \item Yes, everything was clear
    \item Mostly clear, but some aspects were confusing
    \item Neutral
    \item No, the documentation was unclear
    \item No documentation was provided
  \end{Qlist}
}

\Qitem{
  \Qq{Were the example use cases sufficient to understand how to integrate the solver?}
  \begin{Qlist}
    \item Yes, they covered all necessary cases
    \item Somewhat, but additional examples would be helpful
    \item No, they were insufficient
  \end{Qlist}
}

\Qitem{
  \Qq{How easy was it to set up and call the solver in a basic use case?}
  \begin{Qlist}
    \item Very easy (Just a few function calls)
    \item Somewhat easy (Needed minor adjustments)
    \item Neutral
    \item Difficult (Required significant effort)
    \item Very difficult (I could not get it working)
  \end{Qlist}
}

\Qitem{
  \Qq{Did the API provide useful error messages when incorrect inputs were provided?}
  \begin{Qlist}
    \item Yes, the error messages were informative
    \item Somewhat, but could be improved
    \item No, the error messages were vague or missing
  \end{Qlist}
}

\Qitem{
  \Qq{If you encountered any difficulties, what were they? (Open-ended)}
  \Qlines{1}
}

\Qitem{
  \Qq{How would you rate the overall usability of the API?}
  \begin{Qlist}
  \item Excellent
  \item Good
  \item Neutral
  \item Poor
  \item Very Poor
  \end{Qlist}
}

\Qitem{
  \Qq{What improvements would you suggest to enhance the usability of the API? (Open-ended)}
  \Qlines{1}
}


\Qitem{
  \Qq{Would you recommend this solver for integration into a larger numerical computing project?}
  \begin{Qlist}
  \item Yes
  \item No
  \item Maybe, if improvements are made
  \end{Qlist}
}


\end{document}
