\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage{xr,xr-hyper}
\externaldocument{../SRS/SRS}

\usepackage[backend=biber,style=authoryear]{biblatex}
\addbibresource{../../refs/References.bib}

\input{../Comments}
\input{../Common}

\newcounter{testnum} % Test Number
\newcommand{\tthetestnum}{GD\thetestnum}
\newcommand{\tref}[1]{T\ref{#1}}

\newcommand{\rref}[1]{R\ref{#1}}

\begin{document}

\title{System Verification and Validation Plan for \progname{}}
\author{\authname}
\date{\today}

\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{4cm}p{2cm}X}
  \toprule {\bf Date} & {\bf Version} & {\bf Notes}   \\
  \midrule
  24 February 2025    & 1.0           & Initial draft \\
  \bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l}
  \toprule
  \textbf{symbol} & \textbf{description}                     \\
  \midrule
  CI        & Continuous Integration             \\
  MG        & Module Guide                       \\
  MIS       & Module Interface Specification     \\
  SRS       & Software Requirement Specification \\
  T         & Test                               \\
  VnV       & Verification and Validation        \\
  \bottomrule
\end{tabular}

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \cite{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document provides an introductory blurb and roadmap of the Verification and
Validation (VnV) plan

\section{General Information}

This section provides a brief description of the project background and
introduces the objectives for the VnV plan.

\subsection{Summary}

\wss{Say what software is being tested.  Give its name and a brief overview of
  its general functions.}

\progname{} is a sparse linear solver designed to solve large, sparse real
matrices efficiently. It uses the General Minimal Residual (GMRES) method for
internal matrix solves and iterative refinement techniques to improve both speed
and accuracy. The software is intended for use in computational science,
engineering, and numerical analysis applications. As a complete library suite,
the software also includes example programs to demonstrate the solver interfaces
and practical use cases of the solver.

\subsection{Objectives}

\wss{State what is intended to be accomplished.  The objective will be around
  the qualities that are most important for your project.  You might have
  something like: ``build confidence in the software correctness,''
  ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
  just those that are most important.}

\wss{You should also list the objectives that are out of scope. You don't have
  the resources to do everything, so what will you be leaving out. For instance,
  if you are not going to verify the quality of usability, state this. It is
  also worthwhile to justify why the objectives are left out.}

\wss{The objectives are important because they highlight that you are aware of
  limitations in your resources for verification and validation. You can't do
  everything, so what are you going to prioritize? As an example, if your system
  depends on an external library, you can explicitly state that you will assume
  that external library has already been verified by its implementation team.}

The primary objective of the Verification and Validation (VnV) plan is to ensure
the correctness, accuracy, and efficiency of \progname{} in solving sparse linear
systems. The secondary objective is to verify usability and maintainability of the
software for integration with other numerical libraries.

Usability testing for non-expert users is not prioritized is out of the scope of
this VnV plan, as \progname{} is only intended for domain-expert users. The
solver is expected to use an external library for matrix factorization. The
example programs will also depend on an external library for reading and writing
sparse matrices in Matrix Market Exchange Format (\cite{noauthor_matrix_2013}).

\subsection{Challenge Level and Extras}

\wss{State the challenge level (advanced, general, basic) for your project.
Your challenge level should exactly match what is included in your problem
statement.  This should be the challenge level agreed on between you and the
course instructor.  You can use a pull request to update your challenge level
(in TeamComposition.csv or Repos.csv) if your plan changes as a result of the
VnV planning exercise.}

\wss{Summarize the extras (if any) that were tackled by this project.  Extras
can include usability testing, code walkthroughs, user documentation, formal
proof, GenderMag personas, Design Thinking, etc.  Extras should have already
been approved by the course instructor as included in your problem statement.
You can use a pull request to update your extras (in TeamComposition.csv or
Repos.csv) if your plan changes as a result of the VnV planning exercise.}

\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS
  and your other project documents (design documents, like MG, MIS, etc).  You
  can include these even before they are written, since by the time the project
  is done, they will be written.  You can create BibTeX entries for your
  documents and within those entries include a hyperlink to the documents.}

See the Software Requirements Specification (\cite{SRS}) for \progname{}, which
details the goals, requirements, assumptions, and theories of the software. The
Module Guide (\cite{MG}) and Module Interface Specification (\cite{MIS}) document the
design of \progname{}.

\section{Plan}

This section details the plan for the verification of both the documents and the
software for \progname{}. The primary artifacts being verified are: SRS,
software design, VnV Plan, and implementation.

\subsection{Verification and Validation Team}

The table below summarizes the VnV Team for the project:

\vspace{5pt}

\begin{tabularx}{\linewidth}{lX}
  \toprule
  \textbf{Team Member} & \textbf{Role}                           \\
  \midrule
  Xunzhou Ye     & Lead developer and tester         \\
  Qianlin Chen   & ``Domain expert'', provides feedbacks on documents per course
                   guidelines and document templates \\
  Dr. Nedialkov  & Primary stakeholder, oversees project direction and validates
                   all documents                     \\
  \bottomrule
\end{tabularx}

% machine epsilon: the smallest difference in value that the computer can tell
% apart at that given precision

% dependent external library for benchmarking
% google-benchmark for micro-benchmarking, repeats benchmark cases until a
% statistically stable result is obtained.

% why choose runtime?
% The solver is designed to trade computation complexity for space complexity
% Cannot use common complexity analysis, big-O notation to qualify the
% performance of the software.

% Choosing runtime as the only performance metric, open to ideas
% runtime results are unstable because:
% 1. CPU clock frequency, temperature
% 2. OS scheduling, interrupts, context switches
% 3. hardware level optimizations of certain arithmetics at certain precision,
% CPU vs. GPU
% 4. high speed caching
% 5. Inconsistency between runs, hard to reproduce results

\subsection{SRS Verification Plan}

The SRS will be verified via feedbacks from the assigned domain expert and the
project supervisor. A SRS checklist (\cite{SRS_checklist}) will be used to guide
the review process. After each major revision of the SRS, a meeting with the
project supervisor shall be scheduled within two weeks of the revision date.
Changes to the SRS shall be presented and discussed with the supervisor. All
feedbacks from both the domain expert and the supervisor will be documented in
the form of GitHub issues assigned to the project owner. The project owner is
responsible to address and close these issues as they iterate and revise the SRS
document. The key objective is to verify that the software requirements and the
documentation are sound and coherent to the intended users and defined in the
SRS.

\subsection{Design Verification Plan}

Since the software design is derived from a research study, the underlying
algorithms and mathematical models are predefined. As a result, the scope for
structural modifications is limited, and design decisions must align with the
established research framework. The primary focus is on enhancing performance,
ensuring software quality, and maintaining consistency with the research
objectives.

The VnV team will conduct structured reviews of the design, leveraging their
professional expertise to provide informed feedback. This evaluation will be
guided by the MG and MIS checklists (\cite{MG_checklist, MIS_checklist}) to ensure
that design principles are adhered to and best practices are followed. Similar
to the SRS verification process, all feedback will be documented as GitHub
issues to maintain transparency and traceability.

\subsection{Verification and Validation Plan Verification Plan}

The VnV checklist (\cite{VnV_checklist}) will be used to review each iteration of
the VnV Plan. The goal is to uncover any mistakes and reveal any coverage gaps
through the supervision and review of the VnV team members. Once the project
reaches a deliverable milestone, the VnV team will check whether the documented
testing plans and verification processes have been accomplished and the
requirements fulfilled. Feedbacks will again be documented as GitHub issues.

\subsection{Implementation Verification Plan}

Both automated and manual testing will be performed for this project. For
automated static code analysis, linters will be integrated as part of the
Continuous Integration (CI) pipeline via GitHub Actions. Details on the choice
of tools and its use in the project will be discussed in
Section~\ref{sec:autom-test-verif} below. Plans and schemes for automated
dynamic tests will be done at various levels of abstraction, including unit
tests, system tests. Details are listed in Section~\ref{sec:system-tests} and
\ref{sec:unit-test-descr}. A detailed code walkthrough on core algorithms will
be conducted with the project supervisor to ensure that the implementation align
with the established research work that this project is based on.

\subsection{Automated Testing and Verification Tools}
\label{sec:autom-test-verif}

The software is expected to be implemented in C++. The following are the
language specific tools that will be used for automated testing and
verifications:

\begin{itemize}
\item CMake (\cite{noauthor_cmake_nodate}) will be used to streamline the building and
  testing process of the software. CTest, which is part of CMake, will be used
  to generate code coverage reports for unit tests.
\item doctest (\cite{noauthor_doctestdoctest_2025}) will be used as the unit testing
  framework for writing test cases.
\item clang-format (\cite{noauthor_clangformat_nodate}) will be used to format source
  codes based on a set of predefined rules specified in a configuration file. A
  \href{https://git-scm.com/book/ms/v2/Customizing-Git-Git-Hooks}{Git Hook} will
  be deployed to run a format check to ensure that all committed source codes
  are properly formatted. The goal of formatting codes is to improve code
  readability and consistency.
\item clang-tidy (\cite{noauthor_clang-tidy_nodate}) will be used as the linter for
  static code analysis. Committed codes should be free of linter errors and
  warnings to minimize the chance of having incorrect codes.
\item Benchmark (\cite{noauthor_googlebenchmark_2025}) will be used to evaluate the
  runtime performance of the solver as part of the non-functional verification
  process.
\end{itemize}

Apart from these language specific tools, the following general purposed tools
are also used:

\begin{itemize}
\item GitHub Actions will be used as the CI pipeline for the project. Most of the
  language specific automated tools mentioned above will be integrated as part
  of CI to ensure that the verification process is reproducible in an isolated,
  remote environment. This verifies the verification process itself and further
  improves confidence of the verification process.
\end{itemize}

\subsection{Software Validation Plan}

A valid linear solver is one that correctly solves linear systems. In the
context of this project, the validity of the software is primarily determined by
its correctness---ensuring that the computed solutions satisfy the given
equations within an acceptable tolerance. Since this software is part of a
research study, its validity is also assessed by how well the implementation
adheres to the established specifications and aligns with prior research work.
To verify this, code walkthroughs of the core algorithms will be conducted with
the project supervisor.

Beyond correctness, performance also plays a role in validation. The solver is
designed to offer advantages over existing solutions, making performance
benchmarking an essential validation step. To assess this, an automated
performance benchmark will be conducted, generating empirical runtime data for
different problem sizes. These results will be manually compared against
established solvers to evaluate the solverâ€™s computational efficiency.

\section{System Tests}
\label{sec:system-tests}

This section outlines the tests that will be performed for \progname{} to verify
both the functional and non-functional requirements specified in the SRS
(\cite{SRS}). Input specifications and constraints are also listed in the SRS.

\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.}

In this section, the system tests that will be conducted are described in
detail. These tests will be used to verify the fulfillment of the functional
requirements as listed in the SRS (\cite{SRS}).

\subsubsection{Matrix Inputs and Outputs}

This section covers the requirement \rref{R:ex} of the SRS. This includes
essentially a ``driver'' for the solver which loads sparse matrices from a text
file in Matrix Market Exchange (.mtx) Format (\cite{noauthor_matrix_2013}) into memory,
invokes the solver interfaces, and outputs the results returned from the solver.
The tests described below will verify that such ``driver'' is functional.

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:io}:]{matrix-io}

Control: Automatic

Initial State: A hard coded matrix \(\matr{A}\) of size \(\num{100} \times
\num{100}\) in the programming language of choice is instantiated.

Input: matrix \(\matr{A}\) in plain text .mtx format of size \(\num{100} \times
\num{100}\), a random vector \(\vec{b}\) of size \num{100}. \(u_f = u_w = u_r =
\texttt{double}\)

Output: The elements of \(\matr{A}\) matches the hard-coded one. \(\vec{x}\) of size
\num{100}

Test Case Derivation: N/A

How test will be performed: Automatic

\end{itemize}

\subsubsection{Correctness Tests with Manufactured Solutions}

This section covers one of the ways to verify the requirements \rref{R:Axb} and
\rref{R:MP} of the SRS. This includes tests on the accuracy of the yielded
solution from the solver by manufacturing an exact solution \(\vec{x}_\mathrm{ref}\) to the
problem \(\matr{A}\vec{x} = \vec{b}\). This manufacturing process loosely follows the scheme
below:

\begin{enumerate}
\item \(\vec{x}_\mathrm{ref} \gets \text{some random vector}\)
\item \(\vec{b} \gets \matr{A} \vec{x}_\mathrm{ref} \)
\item Solve \(\matr{A}\vec{x} = \vec{b}\)
\item \(e \gets \displaystyle \frac{\norm{\vec{x} - \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2}\)
\end{enumerate}

The relative error \(e\) will be used as the accuracy metric.

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:gdd}:]{generated-double-double}

Control: Automatic

Initial State: matrix \(\matr{A}\) is read from file and stored in memory.
An expected exact solution \(\vec{x}_\mathrm{ref}\) is prepared.

Input: matrix \(\matr{A}\) of size \(\num{10000} \times \num{10000}\) with
\(\cond{\matr{A}} \approx \num{1e2}\), \(\vec{b}\) of size \num{10000}, \(u_f =
u_w = u_r = \texttt{double}\)

Output: \(\vec{x}\) of size \num{10000} such that \(e = \displaystyle \frac{\norm{\vec{x} -
    \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2} < \num{1e-12}\)

Test Case Derivation: \(\vec{x}_\mathrm{ref}\) is randomly generated. \(\vec{b} = \matr{A}\vec{x}_\mathrm{ref}\)

How test will be performed: Automatic

\item[T\refstepcounter{testnum}\thetestnum \label{T:gsd}:]{generated-single-double}

Control: Automatic

Initial State: matrix \(\matr{A}\) is read from file and stored in memory.
An expected exact solution \(\vec{x}_\mathrm{ref}\) is prepared.

Input: matrix \(\matr{A}\) of size \(\num{10000} \times \num{10000}\) with
\(\cond{\matr{A}} \approx \num{1e2}\), \(\vec{b}\) of size \num{10000}, \(u_f = \texttt{single}, u_w
= u_r = \texttt{double}\)

Output: \(\vec{x}\) of size \num{10000} such that \(e = \displaystyle \frac{\norm{\vec{x} -
    \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2} < \num{1e-12}\)

Test Case Derivation: \(\vec{x}_\mathrm{ref}\) is randomly generated. \(\vec{b} =
\matr{A}\vec{x}_\mathrm{ref}\). Matrix factorization is done in single precision while
working and residual computing are done in double precision.

How test will be performed: Automatic

\end{itemize}

\subsubsection{Correctness Tests against Trusted Solvers}

This section covers the other way to verify the requirements \rref{R:Axb} and
\rref{R:MP} of the SRS. This includes tests on the accuracy of the yielded
solution from the solver by comparing it to an external, trusted solver to the
problem \(\matr{A}\vec{x} = \vec{b}\). This process loosely follows the scheme
below:

\begin{enumerate}
\item \(\vec{x}_\mathrm{ref} \gets \text{solution by an external solver}\)
\item Solve \(\matr{A}\vec{x} = \vec{b}\)
\item \(e \gets \displaystyle \frac{\norm{\vec{x} - \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2}\)
\end{enumerate}

The relative error \(e\) will be used as the accuracy metric.

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:exdd}:]{external-double-double}

Control: Automatic

Initial State: matrix \(\matr{A}\) is read from file and stored in memory. The
same problem \(\matr{A}\vec{x} = \vec{b}\) is passed to the external solver. A
reference solution \(\vec{x}_\mathrm{ref}\) is prepared.

Input: matrix \(\matr{A}\) of size \(\num{10000} \times \num{10000}\) with
\(\cond{\matr{A}} \approx \num{1e2}\), \(\vec{b}\) of size \num{10000}, \(u_f =
u_w = u_r = \texttt{double}\)

Output: \(\vec{x}\) of size \num{10000} such that \(e = \displaystyle \frac{\norm{\vec{x} -
    \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2} < \num{1e-12}\)

Test Case Derivation: \(\vec{x}_\mathrm{ref}\) is randomly generated. \(\vec{b} = \matr{A}\vec{x}_\mathrm{ref}\)

How test will be performed: Automatic

\item[T\refstepcounter{testnum}\thetestnum \label{T:exsd}:]{external-single-double}

Control: Automatic

Initial State: matrix \(\matr{A}\) is read from file and stored in memory. The
same problem \(\matr{A}\vec{x} = \vec{b}\) is passed to the external solver. A
reference solution \(\vec{x}_\mathrm{ref}\) is prepared.

Input: matrix \(\matr{A}\) of size \(\num{10000} \times \num{10000}\) with
\(\cond{\matr{A}} \approx \num{1e2}\), \(\vec{b}\) of size \num{10000}, \(u_f = \texttt{single}, u_w
= u_r = \texttt{double}\)

Output: \(\vec{x}\) of size \num{10000} such that \(e = \displaystyle \frac{\norm{\vec{x} -
    \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2} < \num{1e-12}\)

Test Case Derivation: \(\vec{x}_\mathrm{ref}\) is randomly generated. \(\vec{b} =
\matr{A}\vec{x}_\mathrm{ref}\). Matrix factorization is done in single precision while
working and residual computing are done in double precision.

How test will be performed: Automatic

\end{itemize}

\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.  Not all projects will
  necessarily have nonfunctional requirements related to accuracy.}

\wss{For some nonfunctional tests, you won't be setting a target threshold for
passing the test, but rather describing the experiment you will do to measure
the quality for different inputs.  For instance, you could measure speed versus
the problem size.  The output of the test isn't pass/fail, but rather a summary
table or graph.}

\wss{Tests related to usability could include conducting a usability test and
  survey.  The survey will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the
format for the tests given below.}

\wss{If you introduce static tests in your plan, you need to provide details.
How will they be done?  In cases like code (or document) walkthroughs, who will
be involved? Be specific.}

\subsubsection{Accuracy}

The accuracy of the solver will be assessed by verifying that it converges to a
solution within the user-defined tolerance \(\epsilon\). The level of accuracy
required for computational science and engineering applications will be
evaluated through the relative residual norm after convergence. The following
tests will be performed:

\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Type: Functional, Dynamic, Manual, Static etc.

Initial State:

Input/Condition:

Output/Result:

How test will be performed:

\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.

Initial State:

Input:

Output:

How test will be performed:

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Traceability Between Test Cases and Requirements}

% \tref{T:io}
% \tref{T:gdd}
% \tref{T:gsd}
% \tref{T:exdd}
% \tref{T:exsd}
% \rref{R:Axb}
% \rref{R:MP}
% \rref{R:ex}
% \nfrref{NFR:acc}
% \nfrref{NFR:use}
% \nfrref{NFR:mt}
% \nfrref{NFR:port}

% \begin{table}[h!]
%   \centering
%   \begin{tabular}{|c|c|c|}                                      \hline
%   \end{tabular}
%   \caption{Traceability matrix showing the connections between test cases and
%     requirements}
%   \label{Table:T_trace}
% \end{table}

\section{Unit Test Description}
\label{sec:unit-test-descr}

This section should not be filled in until after the MIS (detailed design
  document) has been completed.

% \wss{Reference your MIS (detailed design document) and explain your overall
% philosophy for test case selection.}

% \wss{To save space and time, it may be an option to provide less detail in this section.
% For the unit tests you can potentially layout your testing strategy here.  That is, you
% can explain how tests will be selected for each module.  For instance, your test building
% approach could be test cases for each access program, including one test for normal behaviour
% and as many tests as needed for edge cases.  Rather than create the details of the input
% and output here, you could point to the unit testing code.  For this to work, you code
% needs to be well-documented, with meaningful names for all of the tests.}

% \subsection{Unit Testing Scope}

% \wss{What modules are outside of the scope.  If there are modules that are
%   developed by someone else, then you would say here if you aren't planning on
%   verifying them.  There may also be modules that are part of your software, but
%   have a lower priority for verification than others.  If this is the case,
%   explain your rationale for the ranking of module importance.}

% \subsection{Tests for Functional Requirements}

% \wss{Most of the verification will be through automated unit testing.  If
%   appropriate specific modules can be verified by a non-testing based
%   technique.  That can also be documented in this section.}

% \subsubsection{Module 1}

% \wss{Include a blurb here to explain why the subsections below cover the module.
%   References to the MIS would be good.  You will want tests from a black box
%   perspective and from a white box perspective.  Explain to the reader how the
%   tests were selected.}

% \begin{enumerate}

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}

% Initial State:

% Input:

% Output: \wss{The expected result for the given inputs}

% Test Case Derivation: \wss{Justify the expected value given in the Output field}

% How test will be performed:

% \item{test-id2\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}

% Initial State:

% Input:

% Output: \wss{The expected result for the given inputs}

% Test Case Derivation: \wss{Justify the expected value given in the Output field}

% How test will be performed:

% \item{...\\}

% \end{enumerate}

% \subsubsection{Module 2}

% ...

% \subsection{Tests for Nonfunctional Requirements}

% \wss{If there is a module that needs to be independently assessed for
%   performance, those test cases can go here.  In some projects, planning for
%   nonfunctional tests of units will not be that relevant.}

% \wss{These tests may involve collecting performance data from previously
%   mentioned functional tests.}

% \subsubsection{Module ?}

% \begin{enumerate}

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}

% Initial State:

% Input/Condition:

% Output/Result:

% How test will be performed:

% \item{test-id2\\}

% Type: Functional, Dynamic, Manual, Static etc.

% Initial State:

% Input:

% Output:

% How test will be performed:

% \end{enumerate}

% \subsubsection{Module ?}

% ...

% \subsection{Traceability Between Test Cases and Modules}

% \wss{Provide evidence that all of the modules have been considered.}

% \newpage

\printbibliography{}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\end{document}
