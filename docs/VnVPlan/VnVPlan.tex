\documentclass[12pt, titlepage]{article}

\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage{xr,xr-hyper}
\externaldocument{../SRS/SRS}

\usepackage[backend=biber,style=authoryear]{biblatex}
\addbibresource{../../refs/References.bib}

% Commands for survey template
% https://github.com/annerosenisser/latex-surveys
\usepackage{wasysym}  % provides \ocircle and \Box
\usepackage{enumitem} % easy control of topsep and leftmargin for lists
\usepackage{color}    % used for background color
\usepackage{forloop}  % used for \Qrating and \Qlines
\usepackage{ifthen}   % used for \Qitem and \QItem

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Beginning of questionnaire command definitions %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% 2010, 2012 by Sven Hartenstein
%% mail@svenhartenstein.de
%% http://www.svenhartenstein.de
%%
%% Please be warned that this is NOT a full-featured framework for
%% creating (all sorts of) questionnaires. Rather, it is a small
%% collection of LaTeX commands that I found useful when creating a
%% questionnaire. Feel free to copy and adjust any parts you like.
%% Most probably, you will want to change the commands, so that they
%% fit your taste.
%%
%% Also note that I am not a LaTeX expert! Things can very likely be
%% done much more elegant than I was able to. If you have suggestions
%% about what can be improved please send me an email. I intend to
%% add good tipps to my website and to name contributers of course.
%%
%% 10/2012: Thanks to karathan for the suggestion to put \noindent
%% before \rule!

%% \Qq = Questionaire question. Oh, this is just too simple. It helps
%% making it easy to globally change the appearance of questions.
\newcommand{\Qq}[1]{\textbf{#1}}

%% \QO = Circle or box to be ticked. Used both by direct call and by
%% \Qrating and \Qlist.
\newcommand{\QO}{$\Box$}% or: $\ocircle$

%% \Qrating = Automatically create a rating scale with NUM steps, like
%% this: 0--0--0--0--0.
\newcounter{qr}
\newcommand{\Qrating}[1]{\QO\forloop{qr}{1}{\value{qr} < #1}{---\QO}}

%% \Qline = Again, this is very simple. It helps setting the line
%% thickness globally. Used both by direct call and by \Qlines.
\newcommand{\Qline}[1]{\noindent\rule{#1}{0.6pt}}

%% \Qlines = Insert NUM lines with width=\linewith. You can change the
%% \vskip value to adjust the spacing.
\newcounter{ql}
\newcommand{\Qlines}[1]{\forloop{ql}{0}{\value{ql}<#1}{\vskip0em\Qline{\linewidth}}}

%% \Qlist = This is an environment very similar to itemize but with
%% \QO in front of each list item. Useful for classical multiple
%% choice. Change leftmargin and topsep accourding to your taste.
\newenvironment{Qlist}{%
\renewcommand{\labelitemi}{\QO}
\begin{itemize}[leftmargin=1.5em,topsep=-.5em]
}{%
\end{itemize}
}

%% \Qtab = A "tabulator simulation". The first argument is the
%% distance from the left margin. The second argument is content which
%% is indented within the current row.
\newlength{\qt}
\newcommand{\Qtab}[2]{
\setlength{\qt}{\linewidth}
\addtolength{\qt}{-#1}
\hfill\parbox[t]{\qt}{\raggedright #2}
}

%% \Qitem = Item with automatic numbering. The first optional argument
%% can be used to create sub-items like 2a, 2b, 2c, ... The item
%% number is increased if the first argument is omitted or equals 'a'.
%% You will have to adjust this if you prefer a different numbering
%% scheme. Adjust topsep and leftmargin as needed.
\newcounter{itemnummer}
\newcommand{\Qitem}[2][]{% #1 optional, #2 notwendig
\ifthenelse{\equal{#1}{}}{\stepcounter{itemnummer}}{}
\ifthenelse{\equal{#1}{a}}{\stepcounter{itemnummer}}{}
\begin{enumerate}[topsep=2pt,leftmargin=2.8em]
\item[\textbf{\arabic{itemnummer}#1.}] #2
\end{enumerate}
}

%% \QItem = Like \Qitem but with alternating background color. This
%% might be error prone as I hard-coded some lengths (-5.25pt and
%% -3pt)! I do not yet understand why I need them.
\definecolor{bgodd}{rgb}{0.8,0.8,0.8}
\definecolor{bgeven}{rgb}{0.9,0.9,0.9}
\newcounter{itemoddeven}
\newlength{\gb}
\newcommand{\QItem}[2][]{% #1 optional, #2 notwendig
\setlength{\gb}{\linewidth}
\addtolength{\gb}{-5.25pt}
\ifthenelse{\equal{\value{itemoddeven}}{0}}{%
\noindent\colorbox{bgeven}{\hskip-3pt\begin{minipage}{\gb}\Qitem[#1]{#2}\end{minipage}}%
\stepcounter{itemoddeven}%
}{%
\noindent\colorbox{bgodd}{\hskip-3pt\begin{minipage}{\gb}\Qitem[#1]{#2}\end{minipage}}%
\setcounter{itemoddeven}{0}%
}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% End of questionnaire command definitions %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{../Comments}
\input{../Common}

\newcounter{testnum} % Test Number
\newcommand{\tthetestnum}{GD\thetestnum}
\newcommand{\tref}[1]{T\ref{#1}}

\newcommand{\rref}[1]{R\ref{#1}}
\newcommand{\nfrref}[1]{NFR\ref{#1}}

\begin{document}

\title{System Verification and Validation Plan for \progname{}}
\author{\authname}
\date{\today}

\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{4cm}p{2cm}X}
  \toprule {\bf Date}     & {\bf Version} & {\bf Notes}                   \\
  \midrule
  \date{24 February 2025} & 1.0           & Initial draft                 \\
  \date{11 April 2025}    & 1.1           & Refine according to feedbacks \\
  \bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

% \listoffigures
% \wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l}
  \toprule
  \textbf{symbol} & \textbf{description}                     \\
  \midrule
  API       & Application Programming Interface  \\
  CI        & Continuous Integration             \\
  MG        & Module Guide                       \\
  MIS       & Module Interface Specification     \\
  SRS       & Software Requirement Specification \\
  T         & Test                               \\
  VnV       & Verification and Validation        \\
  \bottomrule
\end{tabular}

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \cite{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

\section{General Information}

This section provides a brief description of the project background and
introduces the objectives for the VnV plan.

\subsection{Summary}

\wss{Say what software is being tested.  Give its name and a brief overview of
  its general functions.}

\progname{} is a sparse linear solver designed to solve large, sparse real
matrices efficiently. It uses the General Minimal Residual (GMRES) method for
internal matrix solves and iterative refinement techniques to improve both speed
and accuracy. The software is intended for use in computational science,
engineering, and numerical analysis applications. As a complete library suite,
the software also includes example programs to demonstrate the solver interfaces
and practical use cases of the solver.

\subsection{Objectives}

\wss{State what is intended to be accomplished.  The objective will be around
  the qualities that are most important for your project.  You might have
  something like: ``build confidence in the software correctness,''
  ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
  just those that are most important.}

\wss{You should also list the objectives that are out of scope. You don't have
  the resources to do everything, so what will you be leaving out. For instance,
  if you are not going to verify the quality of usability, state this. It is
  also worthwhile to justify why the objectives are left out.}

\wss{The objectives are important because they highlight that you are aware of
  limitations in your resources for verification and validation. You can't do
  everything, so what are you going to prioritize? As an example, if your system
  depends on an external library, you can explicitly state that you will assume
  that external library has already been verified by its implementation team.}

The primary objective of the Verification and Validation (VnV) plan is to ensure
the correctness, accuracy, and efficiency of \progname{} in solving sparse linear
systems. The secondary objective is to verify usability and maintainability of the
software for integration with other numerical libraries.

Usability testing for non-expert users is not prioritized is out of the scope of
this VnV plan, as \progname{} is only intended for domain-expert users. The
solver is expected to use an external library for matrix factorization. The
example programs will also depend on an external library for reading and writing
sparse matrices in Matrix Market Exchange Format (\cite{noauthor_matrix_2013}).

\subsection{Challenge Level and Extras}

\wss{State the challenge level (advanced, general, basic) for your project.
Your challenge level should exactly match what is included in your problem
statement.  This should be the challenge level agreed on between you and the
course instructor.  You can use a pull request to update your challenge level
(in TeamComposition.csv or Repos.csv) if your plan changes as a result of the
VnV planning exercise.}

\wss{Summarize the extras (if any) that were tackled by this project.  Extras
can include usability testing, code walkthroughs, user documentation, formal
proof, GenderMag personas, Design Thinking, etc.  Extras should have already
been approved by the course instructor as included in your problem statement.
You can use a pull request to update your extras (in TeamComposition.csv or
Repos.csv) if your plan changes as a result of the VnV planning exercise.}

The challenge level remains the same at the general level. The extra is expected
to be conducting a usability test which will be discussed in
Section~\ref{sec:usability}.

\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS
  and your other project documents (design documents, like MG, MIS, etc).  You
  can include these even before they are written, since by the time the project
  is done, they will be written.  You can create BibTeX entries for your
  documents and within those entries include a hyperlink to the documents.}

See the Software Requirements Specification (\cite{SRS}) for \progname{}, which
details the goals, requirements, assumptions, and theories of the software. The
Module Guide (\cite{MG}) and Module Interface Specification (\cite{MIS}) document the
design of \progname{}.

\section{Plan}

This section details the plan for the verification of both the documents and the
software for \progname{}. The primary artifacts being verified are: SRS,
software design, VnV Plan, and implementation.

\subsection{Verification and Validation Team}

The table below summarizes the VnV Team for the project:

\vspace{5pt}

\begin{tabularx}{\linewidth}{lX}
  \toprule
  \textbf{Team Member} & \textbf{Role}                           \\
  \midrule
  Xunzhou Ye     & Lead developer and tester         \\
  Qianlin Chen   & ``Domain expert'', provides feedbacks on documents per course
                   guidelines and document templates \\
  Dr. Smith      & Course instructor, provides feedbacks on documents per course
                   requirements                      \\
  Dr. Nedialkov  & Primary stakeholder, oversees project direction and validates
                   all documents                     \\
  \bottomrule
\end{tabularx}

% machine epsilon: the smallest difference in value that the computer can tell
% apart at that given precision

% dependent external library for benchmarking
% google-benchmark for micro-benchmarking, repeats benchmark cases until a
% statistically stable result is obtained.

% why choose runtime?
% The solver is designed to trade computation complexity for space complexity
% Cannot use common complexity analysis, big-O notation to qualify the
% performance of the software.

% Choosing runtime as the only performance metric, open to ideas
% runtime results are unstable because:
% 1. CPU clock frequency, temperature
% 2. OS scheduling, interrupts, context switches
% 3. hardware level optimizations of certain arithmetics at certain precision,
% CPU vs. GPU
% 4. high speed caching
% 5. Inconsistency between runs, hard to reproduce results

\subsection{SRS Verification Plan}

The SRS will be verified through an iterative review process with the project
supervisor. The objective is to ensure that the documented software requirements
accurately reflect the project’s goals, are technically sound, and meet quality
standards for scientific software development.

\subsubsection{Supervisor Walkthrough Meetings}
\label{sec:walkthroughmeeting}

After each major revision of the SRS, a walkthrough meeting will be scheduled
within two weeks. The meeting will be led by the author(s) of the SRS and
structured to maximize clarity, engagement, and feedback. To avoid the
inefficiencies of a full line-by-line review, the walkthrough will instead:

\begin{itemize}
\item Begin with a brief overview of the SRS document structure and purpose,
  reaffirming the role of each section in capturing software requirements.
\item Focus the discussion on key functional and non-functional requirements,
  underlying assumptions, and any newly introduced models or scope changes.
\item Use figures, equations, and tables as prompts for discussion and
  clarification, ensuring their interpretation aligns with the supervisor’s
  understanding.
\item Be guided by open-ended questions posed by the author (e.g., “What are your
  thoughts on the maintainability test design?” or “How well does the
  portability requirement reflect our deployment targets?”), rather than binary
  yes/no queries.
\item Encourage clarifications, suggestions, and identification of gaps or
  inconsistencies in the document.
\end{itemize}

\subsubsection{Feedback Management}
\label{sec:feedbackmanagement}

Feedback from the walkthrough will be documented as GitHub issues assigned to
the project owner. Each issue will include a clear summary of the feedback
point, context, and the proposed or required action. The project owner is
responsible for addressing and closing these issues through successive
iterations of the SRS.


\subsection{Design Verification Plan}

The VnV team will conduct structured reviews of the design, leveraging their
professional expertise to provide informed feedback. This evaluation will be
guided by the MG and MIS checklists (\cite{MG_checklist, MIS_checklist}) to ensure
that design principles are adhered to and best practices are followed. Similar
to the SRS verification process, all feedback will be documented as GitHub
issues to maintain transparency and traceability.

\subsection{Verification and Validation Plan Verification Plan}

The VnV checklist (\cite{VnV_checklist}) will be used to review each iteration of
the VnV Plan. The goal is to uncover any mistakes and reveal any coverage gaps
through the supervision and review of the VnV team members. Once the project
reaches a deliverable milestone, the VnV team will check whether the documented
testing plans and verification processes have been accomplished and the
requirements fulfilled. Feedbacks will again be documented as GitHub issues.

\subsection{Implementation Verification Plan}

Both automated and manual testing will be performed for this project. For
automated static code analysis, linters will be integrated as part of the
Continuous Integration (CI) pipeline via GitHub Actions. Details on the choice
of tools and its use in the project will be discussed in
Section~\ref{sec:autom-test-verif} below. Plans and schemes for automated
dynamic tests will be done at various levels of abstraction, including unit
tests, system tests. Details are listed in Section~\ref{sec:system-tests} and
\ref{sec:unit-test-descr}.

In addition to automated testing, a manual code walkthrough will be conducted
with the project supervisor. This walkthrough will serve as a key verification
activity to ensure that the implementation adheres to the research foundation
upon which the project is built. The walkthrough will specifically focus on the
implementation of the core numerical algorithms, including:

\begin{itemize}
\item The iterative refinement process, which is central to the solver's
  mixed-precision accuracy and convergence behavior.
\item The GMRES method, used for solving the inner linear systems during
  refinement.
\end{itemize}

During the walkthrough, the author of the implementation will present the
relevant source code, explain design decisions, and describe how each function
aligns with the intended mathematical formulations. The project supervisor will
review the logic, verify consistency with the theoretical foundations, and
provide direct feedback.

\subsection{Automated Testing and Verification Tools}
\label{sec:autom-test-verif}

The software is expected to be implemented in C++. The following are the
language specific tools that will be used for automated testing and
verifications:

\begin{itemize}
\item CMake (\cite{noauthor_cmake_nodate}) will be used to streamline the building and
  testing process of the software. CTest, which is part of CMake, will be used
  to generate code coverage reports for unit tests.
\item doctest (\cite{noauthor_doctestdoctest_2025}) will be used as the unit testing
  framework for writing test cases.
\item clang-format (\cite{noauthor_clangformat_nodate}) will be used to format source
  codes based on a set of predefined rules specified in a configuration file. A
  \href{https://git-scm.com/book/ms/v2/Customizing-Git-Git-Hooks}{Git Hook} will
  be deployed to run a format check to ensure that all committed source codes
  are properly formatted. The goal of formatting codes is to improve code
  readability and consistency.
\item clang-tidy (\cite{noauthor_clang-tidy_nodate}) will be used as the linter for
  static code analysis. Committed codes should be free of linter errors and
  warnings to minimize the chance of having incorrect codes.
\item Benchmark (\cite{noauthor_googlebenchmark_2025}) will be used to evaluate the
  runtime performance of the solver as part of the nonfunctional verification
  process.
\end{itemize}

Apart from these language specific tools, the following general purposed tools
are also used:

\begin{itemize}
\item GitHub Actions will be used as the CI pipeline for the project. Most of the
  language specific automated tools mentioned above will be integrated as part
  of CI to ensure that the verification process is reproducible in an isolated,
  remote environment. This verifies the verification process itself and further
  improves confidence of the verification process.
\end{itemize}

\subsection{Software Validation Plan}

A valid linear solver must not only produce correct results but also effectively
serve its intended real-world applications. In this project, validation will
focus on demonstrating that the solver meets its intended performance and
usability requirements under realistic conditions.

To validate correctness, the solver will be evaluated on a suite of benchmark
problems that represent typical sparse linear systems encountered in
computational science and engineering. The computed solutions will be compared
against established reference solutions to confirm that they satisfy the
underlying equations within the predefined tolerance.

Performance validation will involve automated benchmarking tests that measure
runtime, memory usage, and scalability across a range of problem sizes. The
solver's performance will be compared against existing solvers, ensuring that
the mixed-precision approach delivers the expected improvements in computational
efficiency.

Dr. Nedialkov has done some investigations on how mixed-precision computing can
reduce computation time and memory usage, and it compares various solvers
against a double-precision baseline (\cite{bassi_investigating_2022}).
Benchmarks used and results from this paper will be used to validate both
solution accuracy and performance of \progname{}.

Finally, validation will extend to acceptance testing in simulated real-world
scenarios. These tests will involve applying the solver to representative use
cases to confirm that its performance and behavior align with the practical
needs of its target users. Feedback from these end-use tests will provide
critical insights into any adjustments required before deployment, ensuring that
the solver is not only correct in a controlled environment but also effective in
practice.

\section{System Tests}
\label{sec:system-tests}

This section outlines the tests that will be performed for \progname{} to verify
both the functional and nonfunctional requirements specified in the SRS
(\cite{SRS}). Input specifications and constraints are also listed in the SRS.

\subsection{Tests for Functional Requirements}
\label{sec:tests-funct-requ}

In this section, the system tests that will be conducted are described in
detail. These tests will be used to verify the fulfillment of the functional
requirements as listed in the SRS (\cite{SRS}).

\subsubsection{Matrix Inputs and Outputs}

This section covers the requirement \rref{R:ex} of the SRS. This includes
essentially a ``driver'' for the solver which loads sparse matrices from a text
file in Matrix Market Exchange (.mtx) Format (\cite{noauthor_matrix_2013}) into memory,
invokes the solver interfaces, and outputs the results returned from the solver.
The tests described below will verify that such a ``driver'' is functional.

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:io}:]{matrix-io}

Control: Automatic

Initial State: Solver and driver initiated.

Input: matrix \(\matr{A}\) in plain text .mtx format of size \(\num{100} \times
\num{100}\), filled from top to bottom, left to right with integers from 1 to
\num{10000}, a random vector \(\vec{b}\) of size \num{100}. \(u_f = u_w = u_r =
\texttt{double}\)

Output: The elements of \(\matr{A}\) matches exactly the one in the .mtx file.
Some result solution \(\vec{x}\) of size \num{100}, values does not matter.

Test Case Derivation: N/A

How test will be performed: Automatic

\end{itemize}

\subsubsection{Correctness Tests with Manufactured Solutions}
\label{sec:corr-tests-with}

This section covers one of the ways to verify the requirements \rref{R:Axb} and
\rref{R:MP} of the SRS. This includes tests on the accuracy of the solution from
the solver by manufacturing an exact solution \(\vec{x}_\mathrm{ref}\) to the
problem \(\matr{A}\vec{x} = \vec{b}\). This manufacturing process loosely
follows the scheme below:

\begin{enumerate}
\item \(\vec{x}_\mathrm{ref} \gets \text{some random vector}\)
\item \(\vec{b} \gets \matr{A} \vec{x}_\mathrm{ref} \)
\item Solve \(\matr{A}\vec{x} = \vec{b}\)
\item \(e \gets \displaystyle \frac{\norm{\vec{x} - \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2}\)
\end{enumerate}

The relative error \(e\) will be used as the accuracy metric. The values of the
manufactured reference solution \(\vec{x}_\mathrm{ref}\) in this section is
uniformly distributed in the range of \([\min(a_{i,j}), \max(a_{i,j})]\). For
the test cases in Sections~\ref{sec:corr-tests-with},
\ref{sec:corr-tests-against}, and below, the \texttt{bundle1} matrix
(\cite{m_lourakis_lourakisbundle1_2006}) from the Florida Sparse Matrix
Collection (\cite{davis_university_2011}) will be used as the input matrix
\(\matr{A}\). This matrix has a size of \(\num{10581} \times \num{10581}\) and
\num{770811} non-zeros. The estimated condition number is \num{1.3e4}.

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:gdd}:]{generated-double-double}

Control: Automatic

Initial State: matrix \(\matr{A}\) is read from file and stored in memory.
An expected exact solution \(\vec{x}_\mathrm{ref}\) and the corresponding \(\vec{b} =
\matr{A}\vec{x}_\mathrm{ref}\) are prepared.

Input: \(\matr{A}, \vec{b}, u_f = u_w = u_r = \texttt{double}\)

Output: \(\vec{x}\) such that \(e = \displaystyle \frac{\norm{\vec{x} -
    \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2} < \num{1e-10}\)

Test Case Derivation: \(\vec{x}_\mathrm{ref}\) is randomly generated. \(\vec{b} = \matr{A}\vec{x}_\mathrm{ref}\)

How test will be performed: Automatic

\item[T\refstepcounter{testnum}\thetestnum \label{T:gsd}:]{generated-single-double}

Control: Automatic

Initial State: matrix \(\matr{A}\) is read from file and stored in memory.
An expected exact solution \(\vec{x}_\mathrm{ref}\) and the corresponding \(\vec{b} =
\matr{A}\vec{x}_\mathrm{ref}\) are prepared.

Input: \(\matr{A}, \vec{b}, u_f = \texttt{single}, u_w = u_r = \texttt{double}\)

Output: \(\vec{x}\) such that \(e = \displaystyle \frac{\norm{\vec{x} -
    \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2} < \num{1e-10}\)

Test Case Derivation: \(\vec{x}_\mathrm{ref}\) is randomly generated. \(\vec{b} =
\matr{A}\vec{x}_\mathrm{ref}\). Matrix factorization is done in \texttt{single}
precision while working and residual evaluations are done in \texttt{double}
precision.

How test will be performed: Automatic

\end{itemize}

\subsubsection{Correctness Tests against Trusted Solvers}
\label{sec:corr-tests-against}

This section covers the other way to verify the requirements \rref{R:Axb} and
\rref{R:MP} of the SRS. This includes tests on the accuracy of the yielded
solution from the solver by comparing it to an external, trusted solver to the
problem \(\matr{A}\vec{x} = \vec{b}\). This process loosely follows the scheme
below:

\begin{enumerate}
\item \(\vec{x}_\mathrm{ref} \gets \text{solution by an external solver}\)
\item Solve \(\matr{A}\vec{x} = \vec{b}\)
\item \(e \gets \displaystyle \frac{\norm{\vec{x} - \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2}\)
\end{enumerate}

The relative error \(e\) will be used as the accuracy metric. For the test cases
in this Section, MATLAB\textsuperscript{\textregistered} will be used as the
external reference solver.

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:exdd}:]{external-double-double}

Control: Automatic

Initial State: matrix \(\matr{A}\) is read from file and stored in memory. The
same problem \(\matr{A}\vec{x} = \vec{b}\) is passed to the external solver. A
reference solution \(\vec{x}_\mathrm{ref}\) is prepared.

Input: \(\matr{A}, \vec{b}, u_f = u_w = u_r = \texttt{double}\)

Output: \(\vec{x}\) such that \(e = \displaystyle \frac{\norm{\vec{x} -
    \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2} < \num{1e-10}\)

Test Case Derivation: \(\vec{x}_\mathrm{ref}\) is randomly generated.

How test will be performed: Automatic

\item[T\refstepcounter{testnum}\thetestnum \label{T:exsd}:]{external-single-double}

Control: Automatic

Initial State: matrix \(\matr{A}\) is read from file and stored in memory. The
same problem \(\matr{A}\vec{x} = \vec{b}\) is passed to the external solver. A
reference solution \(\vec{x}_\mathrm{ref}\) is prepared.

Input: \(\matr{A}, \vec{b}, u_f = \texttt{single}, u_w = u_r = \texttt{double}\)

Output: \(\vec{x}\) such that \(e = \displaystyle \frac{\norm{\vec{x} -
    \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2} < \num{1e-10}\)

Test Case Derivation: \(\vec{x}_\mathrm{ref}\) is randomly generated. Matrix factorization is
done in single precision while working and residual evaluations are done in
double precision.

How test will be performed: Automatic

\end{itemize}

\subsection{Tests for Nonfunctional Requirements}

\wss{For some nonfunctional tests, you won't be setting a target threshold for
passing the test, but rather describing the experiment you will do to measure
the quality for different inputs.  For instance, you could measure speed versus
the problem size.  The output of the test isn't pass/fail, but rather a summary
table or graph.}

\wss{Tests related to usability could include conducting a usability test and
  survey.  The survey will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the
format for the tests given below.}

\wss{If you introduce static tests in your plan, you need to provide details.
How will they be done?  In cases like code (or document) walkthroughs, who will
be involved? Be specific.}

In this section, the system tests that will be conducted are described in
detail. These tests will be used to verify the fulfillment of the nonfunctional
requirements as listed in the SRS (\cite{SRS}).

\subsubsection{Accuracy}

The accuracy of the solver will be assessed by verifying that it converges to a
solution within the user-defined tolerance \(\epsilon\). The level of accuracy
required for computational science and engineering applications will be
evaluated through the relative residual norm after convergence. The functional
tests \tref{T:gdd}, \tref{T:gsd}, \tref{T:exdd}, \tref{T:exsd} are sufficient to
verify the nonfunctional requirement \nfrref{NFR:acc} in the SRS with an
accuracy metric of \(\epsilon \approx \num{1e-10}\). Considering that the
residual precision \(u_r = \texttt{double}\), the chosen \(\epsilon \approx
\num{1e-10}\) is reasonably close to the machine epsilon in \texttt{double}
precise \(\epsilon_\mathrm{mach} \approx \num{1.1e-16}\).

% \begin{itemize}

% \item[T\refstepcounter{testnum}\thetestnum \label{T:acc}:]{nfr-acc}

% Type: Functional, Automated

% Initial State: Similar to \tref{T:gdd}, matrix \(\matr{A}\) is read from file
% and stored in memory. An manufactured expected exact solution
% \(\vec{x}_\mathrm{ref}\) is prepared.

% Input: Similar to \tref{T:gdd}, with an extra input for user-specified tolerance
% \(\epsilon = \num{1e-12}\).

% Output/Result: \(\vec{x}\) of size \num{10000} such that \(e = \displaystyle
% \frac{\norm{\vec{x} - \vec{x}_\mathrm{ref}}_2}{\norm{\vec{x}_\mathrm{ref}}_2} <
% \epsilon = \num{1e-12}\). The computed solution’s residual norm should be below
% the threshold \(\epsilon\). If convergence is not achieved after iterations, a
% warning should be issued.

% How test will be performed: Automatic

% \end{itemize}

\subsubsection{Usability}
\label{sec:usability}

The usability of the solver will be evaluated based on the clarity and
accessibility of its public Application Programming Interface (API). The API
should be self-contained, readable, and easy to integrate into other software as
a dependency. Usability testing will reference the user characteristics section
and include developer feedback. The following tests will be performed to verify
the nonfunctional requirement \nfrref{NFR:use} in the SRS:

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:use}:]{nfr-use}

Type: Static, Review, Survey-Based

Initial State: The API documentation and usage examples are available. The API
documentation of an external, established sparse linear solver is also presented
as a comparable reference. Here the sparse solvers concept from the Eigen
library is used as the reference (\cite{noauthor_eigen_nodate}).

Input: A detailed usability survey questionnaire will be distributed to all VnV
team members. The survey will include both open-ended and Likert-scale questions
focused on API clarity, ease of integration, and documentation quality. VnV team
members with expertise in numerical software development will serve as the
primary reviewers.

Output/Result: A consolidated report of survey responses will summarize feedback
on the API’s clarity, ease of integration, advantages and drawbacks, and provide
specific suggestions for improvements. The report will highlight areas for
enhancement based on a comparative analysis with the external reference API.

How test will be performed:
\begin{enumerate}
\item The VnV team will first independently review the API documentation and usage
  examples.
\item Each reviewer will complete the usability survey (see
  Appendix~\ref{sec:usab-surv-quest}).
\item After submitting the survey, a follow-up meeting will be scheduled where each
  reviewer presents their feedback, and common themes and specific issues are
  discussed.
\item The discussion and agreed-upon action items will be documented, and
  suggestions for improvements will be tracked via GitHub issues.
\end{enumerate}

\end{itemize}

\subsubsection{Maintainability}

The effort required to modify the solver should be kept minimal. This will be
verified by estimating the complexity of implementing likely changes and
ensuring that modifications take less than a fraction of the original
development time. The following tests will be performed to verify the
nonfunctional requirement \nfrref{NFR:mt} in the SRS:

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:mt}:]{nfr-mt}

Type: Static, Code Review

Initial State: The codebase is available and structured.

Input: Introduce an API extension to support solving matrices stored in
Compressed Sparse Row (CSR) format in addition to the existing support for
Compressed Sparse Column (CSC) format.

Output/Result: Measure the number of files and lines of code modified, as well
as time taken.

How test will be performed:

\begin{enumerate}
\item Before starting, the current state of the codebase will be documented. This
  includes recording the total number of files, the overall lines of code (LOC),
  and any existing metrics related to code complexity. These baseline
  measurements serve as a reference for evaluating the changes.
\item The developer will work on introducing the new CSR support. During this
  phase, the developer is expected to follow established coding standards and
  utilize existing modules to minimize unnecessary modifications. The process
  should include:
  \begin{itemize}
  \item Designing the API changes needed to accommodate CSR format.
  \item Modifying relevant modules to handle both CSR and CSC formats where
    applicable.
  \item Integrating new code with the current codebase using modular and reusable
    practices.
  \end{itemize}
\item After the API extension is implemented, a code review will be conducted. The
  review team will assess the following:
  \begin{itemize}
  \item The number of files modified.
  \item The total lines of code added, removed, or altered.
  \item The time taken from the start of the implementation to the final
    integration.
  \item An evaluation of the complexity changes introduced by the new code.
  \end{itemize}
\item The measured modifications and time are then compared against the predefined
  threshold (\hyperref[tab:symbolic]{FRACTION} of the original development
  time). If the modifications exceed the acceptable limits, this could indicate
  potential maintainability issues that need to be addressed.
\item All findings will be documented in detail, and the results will be discussed
  in a follow-up review meeting. Any issues or suggestions for reducing the
  modification effort will be recorded as GitHub issues for further refinement
  of the codebase.
\end{enumerate}

\end{itemize}

\subsubsection{Portability}

The solver should run on all actively maintained operating systems, including
Windows 10, Windows 11, Linux, and MacOS. Compatibility testing will verify that
all required functionalities work across different platforms. The following
tests will be performed to verify the nonfunctional requirement \nfrref{NFR:port}
in the SRS:

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:port}:]{nfr-port}

Type: Functional, Automated

Initial State: A Linux machine loaded with Docker images of the latest version
of Windows 11, Ubuntu LTS (Linux), and MacOS.

Input: Compile both the solver and the example program in each virtual
environment. Run both the example program and all automated functional tests
described in Section~\ref{sec:tests-funct-requ}.

Output/Result: The solver successfully compiles, executes test cases, and
produces correct results.

How test will be performed: Automated CI pipelines will test the software across
different platforms. Success will be determined by running the test suite in
each environment and confirming consistent results.

\end{itemize}


\subsubsection{Performance}

To fulfill the nonfunctional requirement for performance \nfrref{NFR:perf}, a
manual benchmark test will be conducted to evaluate the solver’s runtime
efficiency when using mixed-precision arithmetic. The runtime of the functional
tests \tref{T:gdd} and \tref{T:gsd} will serve as the basis for comparing
performance between two configurations:
\begin{enumerate}
\item Mixed-precision mode: factorization in single precision, internal solves and
  residual evaluations in double precision.
\item Double-precision mode: all computations in double precision.
\end{enumerate}
Each test will be executed multiple times to account for variability and ensure
statistically stable results. This is necessary because clock-time measurements
on modern processors can be affected by factors such as CPU caching, thermal
throttling, and OS-level scheduling.

\begin{itemize}

\item[T\refstepcounter{testnum}\thetestnum \label{T:perf}:]{nfr-perf}

Type: Functional, Manual, Benchmark-Based

Initial State: As described in \tref{T:gdd} and \tref{T:gsd}. Benchmarking tools
with high-resolution timing functions (e.g., std::chrono in C++) are available
for accurate runtime measurements.

Input: Manually run the solver in both configurations on the selected matrix.
The matrix should be solved multiple times (e.g., 10 runs per configuration) to
account for performance variability.

Output/Result: Record the average runtime of each configuration for each matrix.
The mixed-precision version should demonstrate a consistent runtime reduction of
at least \hyperref[tab:symbolic]{PERF\_GAIN}. Results can be presented in a
comparison chart.

How test will be performed:

The benchmark will be conducted on a single machine to ensure consistency. Prior
to testing, CPU frequency scaling will be disabled (e.g., using
\texttt{cpufreq-set -g performance} on Linux), background processes will be
minimized, and the solver will be pinned to a single CPU core using tools such
as \texttt{taskset} to reduce variability from OS-level scheduling. Before
measurement, several warm-up runs will be performed in each configuration to
stabilize caching behavior and eliminate anomalies from first-time execution.
The solver will then be run 10 times on a single representative sparse matrix
using both the mixed-precision configuration (single precision for
factorization, double precision for solves and residuals) and the full
double-precision configuration. Runtime for each run will be measured using
embedded high-resolution timers. The average runtime and standard deviation for
each configuration will be calculated and compared. A consistent runtime
reduction in the mixed-precision mode will confirm that the performance
requirement is met. The results will be visualized in a plot to clearly
illustrate the performance difference between the two configurations.

\end{itemize}

\subsection{Traceability Between Test Cases and Requirements}

% \tref{T:io}
% \tref{T:gdd}
% \tref{T:gsd}
% \tref{T:exdd}
% \tref{T:exsd}
% \tref{T:use}
% \tref{T:mt}
% \tref{T:fmt}
% \tref{T:port}
% \rref{R:Axb}
% \rref{R:MP}
% \rref{R:ex}
% \nfrref{NFR:acc}
% \nfrref{NFR:use}
% \nfrref{NFR:mt}
% \nfrref{NFR:port}
% \nfrref{NFR:perf}

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|}                                      \hline
                  & \rref{R:Axb} & \rref{R:MP} & \rref{R:ex} & \nfrref{NFR:acc} & \nfrref{NFR:use} & \nfrref{NFR:mt} & \nfrref{NFR:port} & \nfrref{NFR:perf} \\ \hline
    \tref{T:io}   &              &             & X           &                  &                  &                 &                   &                   \\ \hline
    \tref{T:gdd}  & X            & X           &             & X                &                  &                 &                   &                   \\ \hline
    \tref{T:gsd}  & X            & X           &             & X                &                  &                 &                   &                   \\ \hline
    \tref{T:exdd} & X            & X           &             & X                &                  &                 &                   &                   \\ \hline
    \tref{T:exsd} & X            & X           &             & X                &                  &                 &                   &                   \\ \hline
    \tref{T:use}  &              &             &             &                  & X                &                 &                   &                   \\ \hline
    \tref{T:mt}   &              &             &             &                  &                  & X               &                   &                   \\ \hline
    \tref{T:port} &              &             &             &                  &                  &                 & X                 &                   \\ \hline
    \tref{T:perf} &              &             &             &                  &                  &                 &                   & X                 \\ \hline
  \end{tabular}
  \caption{Traceability matrix showing the connections between test cases and
    requirements}
  \label{Table:T_trace}
\end{table}

\section{Unit Test Description}
\label{sec:unit-test-descr}

This section should not be filled in until after the MIS (detailed design
  document) has been completed.

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}

\wss{To save space and time, it may be an option to provide less detail in this section.
For the unit tests you can potentially layout your testing strategy here.  That is, you
can explain how tests will be selected for each module.  For instance, your test building
approach could be test cases for each access program, including one test for normal behaviour
and as many tests as needed for edge cases.  Rather than create the details of the input
and output here, you could point to the unit testing code.  For this to work, you code
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}

Initial State:

Input:

Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed:

\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}

Initial State:

Input:

Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed:

\item{...\\}

\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}

Initial State:

Input/Condition:

Output/Result:

How test will be performed:

\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.

Initial State:

Input:

Output:

How test will be performed:

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}

\newpage

\printbibliography{}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\renewcommand{\arraystretch}{1.2}
\begin{table}[H]
  \centering
  \begin{tabular}{ll}
    \toprule
    \textbf{symbol}  & \textbf{value}          \\
    \midrule
    FRACTION   & \SI{30}{\percent} \\
    PERF\_GAIN & \SI{10}{\percent} \\
    \bottomrule
  \end{tabular}
  \caption{Symbolic Parameters}
  \label{tab:symbolic}
\end{table}

\subsection{Usability Survey Questions}
\label{sec:usab-surv-quest}

\Qitem{
  \Qq{Did you find the function names and parameters intuitive?}
  \begin{Qlist}
  \item Very intuitive
  \item Somewhat intuitive
  \item Neutral
  \item Confusing
  \item Very confusing
  \end{Qlist}
}

\Qitem{
  \Qq{Did the provided documentation clearly explain how to use the API?}
  \begin{Qlist}
    \item Yes, everything was clear
    \item Mostly clear, but some aspects were confusing
    \item Neutral
    \item No, the documentation was unclear
    \item No documentation was provided
  \end{Qlist}
}

\Qitem{
  \Qq{Were the example use cases sufficient to understand how to integrate the solver?}
  \begin{Qlist}
    \item Yes, they covered all necessary cases
    \item Somewhat, but additional examples would be helpful
    \item No, they were insufficient
  \end{Qlist}
}

\Qitem{
  \Qq{How easy was it to set up and call the solver in a basic use case?}
  \begin{Qlist}
    \item Very easy (Just a few function calls)
    \item Somewhat easy (Needed minor adjustments)
    \item Neutral
    \item Difficult (Required significant effort)
    \item Very difficult (I could not get it working)
  \end{Qlist}
}

\Qitem{
  \Qq{Did the API provide useful error messages when incorrect inputs were provided?}
  \begin{Qlist}
    \item Yes, the error messages were informative
    \item Somewhat, but could be improved
    \item No, the error messages were vague or missing
  \end{Qlist}
}

\Qitem{
  \Qq{If you encountered any difficulties, what were they? (Open-ended)}
  \Qlines{1}
}

\Qitem{
  \Qq{How would you rate the overall usability of the API?}
  \begin{Qlist}
  \item Excellent
  \item Good
  \item Neutral
  \item Poor
  \item Very Poor
  \end{Qlist}
}

\Qitem{
  \Qq{What improvements would you suggest to enhance the usability of the API? (Open-ended)}
  \Qlines{1}
}


\Qitem{
  \Qq{Would you recommend this solver for integration into a larger numerical computing project?}
  \begin{Qlist}
  \item Yes
  \item No
  \item Maybe, if improvements are made
  \end{Qlist}
}


\end{document}
